{
  "selectedFiles": [],
  "selectedURLs": [],
  "selectedNotes": [],
  "selectedSpecials": [
    [
      "selection:1731167960599",
      {
        "icon": "✂️",
        "name": "Selected Content from https://platform.openai.com/docs/api-reference/debugging-requests"
      }
    ],
    [
      "selection:1731168038916",
      {
        "icon": "✂️",
        "name": "Selected Content from https://platform.openai.com/docs/api-reference/chat/create"
      }
    ],
    [
      "selection:1731168187467",
      {
        "icon": "✂️",
        "name": "Selected Content from https://platform.openai.com/docs/api-reference/images"
      }
    ],
    [
      "selection:1731168264202",
      {
        "icon": "✂️",
        "name": "Selected Content from https://platform.openai.com/docs/quickstart"
      }
    ],
    [
      "selection:1731168348570",
      {
        "icon": "✂️",
        "name": "Selected Content from https://platform.openai.com/docs/models#models-overview"
      }
    ],
    [
      "selection:1731168383169",
      {
        "icon": "✂️",
        "name": "Selected Content from https://platform.openai.com/docs/guides/text-generation"
      }
    ],
    [
      "selection:1731168392564",
      {
        "icon": "✂️",
        "name": "Selected Content from https://platform.openai.com/docs/guides/images"
      }
    ],
    [
      "selection:1731168512635",
      {
        "icon": "✂️",
        "name": "Selected Content from https://platform.openai.com/docs/guides/structured-outputs"
      }
    ]
  ],
  "outputContents": [
    [
      "selection:1731167960599",
      "Selected content from https://platform.openai.com/docs/api-reference/debugging-requests on 2024-11-09 15:59:20\n\nIntroduction\nYou can interact with the API through HTTP requests from any language, via our official Python bindings, our official Node.js library, or a community-maintained library.\n\nTo install the official Python bindings, run the following command:\n\n\npip install openai\nTo install the official Node.js library, run the following command in your Node.js project directory:\n\n\nnpm install openai\nAuthentication\nAPI keys\nThe OpenAI API uses API keys for authentication. You can create API keys at a user or service account level. Service accounts are tied to a \"bot\" individual and should be used to provision access for production systems. Each API key can be scoped to one of the following,\n\nProject keys - Provides access to a single project (preferred option); access Project API keys by selecting the specific project you wish to generate keys against.\nUser keys - Our legacy keys. Provides access to all organizations and all projects that user has been added to; access API Keys to view your available keys. We highly advise transitioning to project keys for best security practices, although access via this method is currently still supported.\nRemember that your API key is a secret! Do not share it with others or expose it in any client-side code (browsers, apps). Production requests must be routed through your own backend server where your API key can be securely loaded from an environment variable or key management service.\n\nAll API requests should include your API key in an Authorization HTTP header as follows:\n\n\nAuthorization: Bearer OPENAI_API_KEY\nOrganizations and projects (optional)\nFor users who belong to multiple organizations or are accessing their projects through their legacy user API key, you can pass a header to specify which organization and project is used for an API request. Usage from these API requests will count as usage for the specified organization and project.\n\nTo access the Default project in an organization, leave out the OpenAI-Project header\n\nExample curl command:\n\n\n1\n2\n3\n4\ncurl https://api.openai.com/v1/models \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"OpenAI-Organization: org-ujzrKXfuqUZuwmOimFii0ZfS\" \\\n  -H \"OpenAI-Project: $PROJECT_ID\"\nExample with the openai Python package:\n\n\n1\n2\n3\n4\n5\n6\nfrom openai import OpenAI\n\nclient = OpenAI(\n  organization='org-ujzrKXfuqUZuwmOimFii0ZfS',\n  project='$PROJECT_ID',\n)\nExample with the openai Node.js package:\n\n\n1\n2\n3\n4\n5\n6\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI({\n    organization: \"org-ujzrKXfuqUZuwmOimFii0ZfS\",\n    project: \"$PROJECT_ID\",\n});\nOrganization IDs can be found on your Organization settings page. Project IDs can be found on your General settings page by selecting the specific project.\n\nMaking requests\nYou can paste the command below into your terminal to run your first API request. Make sure to replace $OPENAI_API_KEY with your secret API key. If you are using a legacy user key and you have multiple projects, you will also need to specify the Project Id. For improved security, we recommend transitioning to project based keys instead.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n     \"model\": \"gpt-4o-mini\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n     \"temperature\": 0.7\n   }'\nThis request queries the gpt-4o-mini model (which under the hood points to a gpt-4o-mini model variant) to complete the text starting with a prompt of \"Say this is a test\". You should get a response back that resembles the following:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n{\n    \"id\": \"chatcmpl-abc123\",\n    \"object\": \"chat.completion\",\n    \"created\": 1677858242,\n    \"model\": \"gpt-4o-mini\",\n    \"usage\": {\n        \"prompt_tokens\": 13,\n        \"completion_tokens\": 7,\n        \"total_tokens\": 20,\n        \"completion_tokens_details\": {\n            \"reasoning_tokens\": 0,\n            \"accepted_prediction_tokens\": 0,\n            \"rejected_prediction_tokens\": 0\n        }\n    },\n    \"choices\": [\n        {\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \"\\n\\nThis is a test!\"\n            },\n            \"logprobs\": null,\n            \"finish_reason\": \"stop\",\n            \"index\": 0\n        }\n    ]\n}\nNow that you've generated your first chat completion, let's break down the response object. We can see the finish_reason is stop which means the API returned the full chat completion generated by the model without running into any limits. In the choices list, we only generated a single message but you can set the n parameter to generate multiple messages choices.\n\nStreaming\nThe OpenAI API provides the ability to stream responses back to a client in order to allow partial results for certain requests. To achieve this, we follow the Server-sent events standard. Our official Node and Python libraries include helpers to make parsing these events simpler.\n\nStreaming is supported for both the Chat Completions API and the Assistants API. This section focuses on how streaming works for Chat Completions. Learn more about how streaming works in the Assistants API here.\n\nIn Python, a streaming request looks like:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n    stream=True,\n)\nfor chunk in stream:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\")\nIn Node / Typescript, a streaming request looks like:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n    const stream = await openai.chat.completions.create({\n        model: \"gpt-4o-mini\",\n        messages: [{ role: \"user\", content: \"Say this is a test\" }],\n        stream: true,\n    });\n    for await (const chunk of stream) {\n        process.stdout.write(chunk.choices[0]?.delta?.content || \"\");\n    }\n}\n\nmain();\nParsing Server-sent events\nParsing Server-sent events is non-trivial and should be done with caution. Simple strategies like splitting by a new line may result in parsing errors. We recommend using existing client libraries when possible.\n\nDebugging requests\nIn addition to error codes returned from API responses, it may sometimes be necessary to inspect HTTP response headers as well. Of particular interest will be the headers which contain the unique ID of a particular API request, and information about rate limiting applied to your requests. Below is an incomplete list of HTTP headers returned with API responses:\n\nAPI meta information\n\nopenai-organization: The organization associated with the request\nopenai-processing-ms: Time taken processing your API request\nopenai-version: REST API version used for this request (currently 2020-10-01)\nx-request-id: Unique identifier for this API request (used in troubleshooting)\nRate limiting information\n\nx-ratelimit-limit-requests\nx-ratelimit-limit-tokens\nx-ratelimit-remaining-requests\nx-ratelimit-remaining-tokens\nx-ratelimit-reset-requests\nx-ratelimit-reset-tokens\nOpenAI recommends logging request IDs in production deployments, which will allow more efficient troubleshooting with our support team should the need arise. Our official SDKs provide a property on top level response objects containing the value of the x-request-id header.\n\nRequest ID in Python\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-4o-mini\",\n)\n\nprint(response._request_id)\nRequest ID in JavaScript\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nimport OpenAI from 'openai';\nconst client = new OpenAI();\n\nconst response = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n    model: 'gpt-4o-mini'\n});\n\nconsole.log(response._request_id);\nAccess raw response objects in SDKs\nIf you are using a lower-level HTTP client (like fetch or HttpClient in C#), you should already have access to response headers as a part of the HTTP interface.\n\nIf you are using one of OpenAI's official SDKs (which largely abstract the HTTP request/response cycle), you will need to access raw HTTP responses in a slightly different way.\n\nBelow is an example of accessing the raw response object (and the x-ratelimit-limit-tokens header) using our Python SDK.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.with_raw_response.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-4o-mini\",\n)\nprint(response.headers.get('x-ratelimit-limit-tokens'))\n\n# get the object that `chat.completions.create()` would have returned\ncompletion = response.parse()\nprint(completion)\nHere is how you'd access a raw response (and the x-ratelimit-limit-tokens header) using our JavaScript SDK.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nimport OpenAI from 'openai';\nconst client = new OpenAI();\n\nconst response = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n    model: 'gpt-4o-mini'\n}).asResponse();\n\n// access the underlying Response object\nconsole.log(response.headers.get('x-ratelimit-limit-tokens'))\n\nEnd of selected content from https://platform.openai.com/docs/api-reference/debugging-requests"
    ],
    [
      "selection:1731168038916",
      "Selected content from https://platform.openai.com/docs/api-reference/chat/create on 2024-11-09 16:00:38\n\nChat\nGiven a list of messages comprising a conversation, the model will return a response. Related guide: Chat Completions\n\nCreate chat completion\nPOST\n \nhttps://api.openai.com/v1/chat/completions\nCreates a model response for the given chat conversation. Learn more in the text generation, vision, and audio guides.\n\nRequest body\nmessages\narray\n\nRequired\nA list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, images, and audio.\n\n\nShow possible types\nmodel\nstring\n\nRequired\nID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API.\n\nstore\nboolean or null\n\nOptional\nDefaults to false\nWhether or not to store the output of this chat completion request for use in our model distillation or evals products.\n\nmetadata\nobject or null\n\nOptional\nDeveloper-defined tags and values used for filtering completions in the dashboard.\n\nfrequency_penalty\nnumber or null\n\nOptional\nDefaults to 0\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\nSee more information about frequency and presence penalties.\n\nlogit_bias\nmap\n\nOptional\nDefaults to null\nModify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nlogprobs\nboolean or null\n\nOptional\nDefaults to false\nWhether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.\n\ntop_logprobs\ninteger or null\n\nOptional\nAn integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n\nmax_tokens\nDeprecated\ninteger or null\n\nOptional\nThe maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.\n\nThis value is now deprecated in favor of max_completion_tokens, and is not compatible with o1 series models.\n\nmax_completion_tokens\ninteger or null\n\nOptional\nAn upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n\nn\ninteger or null\n\nOptional\nDefaults to 1\nHow many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.\n\nmodalities\narray or null\n\nOptional\nOutput types that you would like the model to generate for this request. Most models are capable of generating text, which is the default:\n\n[\"text\"]\n\nThe gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n\n[\"text\", \"audio\"]\n\nprediction\nobject\n\nOptional\nConfiguration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n\n\nShow possible types\naudio\nobject or null\n\nOptional\nParameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more.\n\n\nShow properties\npresence_penalty\nnumber or null\n\nOptional\nDefaults to 0\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\nSee more information about frequency and presence penalties.\n\nresponse_format\nobject\n\nOptional\nAn object specifying the format that the model must output. Compatible with GPT-4o, GPT-4o mini, GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.\n\nSetting to { \"type\": \"json_schema\", \"json_schema\": {...} } enables Structured Outputs which ensures the model will match your supplied JSON schema. Learn more in the Structured Outputs guide.\n\nSetting to { \"type\": \"json_object\" } enables JSON mode, which ensures the message the model generates is valid JSON.\n\nImportant: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.\n\n\nShow possible types\nseed\ninteger or null\n\nOptional\nThis feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.\n\nservice_tier\nstring or null\n\nOptional\nDefaults to auto\nSpecifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:\n\nIf set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.\nIf set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.\nIf set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.\nWhen not set, the default behavior is 'auto'.\nWhen this parameter is set, the response body will include the service_tier utilized.\n\nstop\nstring / array / null\n\nOptional\nDefaults to null\nUp to 4 sequences where the API will stop generating further tokens.\n\nstream\nboolean or null\n\nOptional\nDefaults to false\nIf set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Example Python code.\n\nstream_options\nobject or null\n\nOptional\nDefaults to null\nOptions for streaming response. Only set this when you set stream: true.\n\n\nShow properties\ntemperature\nnumber or null\n\nOptional\nDefaults to 1\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or top_p but not both.\n\ntop_p\nnumber or null\n\nOptional\nDefaults to 1\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or temperature but not both.\n\ntools\narray\n\nOptional\nA list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n\n\nShow properties\ntool_choice\nstring or object\n\nOptional\nControls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n\nnone is the default when no tools are present. auto is the default if tools are present.\n\n\nShow possible types\nparallel_tool_calls\nboolean\n\nOptional\nDefaults to true\nWhether to enable parallel function calling during tool use.\n\nuser\nstring\n\nOptional\nA unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.\n\nfunction_call\nDeprecated\nstring or object\n\nOptional\nDeprecated in favor of tool_choice.\n\nControls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function. Specifying a particular function via {\"name\": \"my_function\"} forces the model to call that function.\n\nnone is the default when no functions are present. auto is the default if functions are present.\n\n\nShow possible types\nfunctions\nDeprecated\narray\n\nOptional\nDeprecated in favor of tools.\n\nA list of functions the model may generate JSON inputs for.\n\n\nShow properties\nReturns\nReturns a chat completion object, or a streamed sequence of chat completion chunk objects if the request is streamed.\n\n\nDefault\n\nImage input\n\nStreaming\n\nFunctions\n\nLogprobs\nExample request\ngpt-4o\n\nnode.js\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    messages: [{ role: \"system\", content: \"You are a helpful assistant.\" }],\n    model: \"gpt-4o\",\n  });\n\n  console.log(completion.choices[0]);\n}\n\nmain();\nResponse\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n{\n  \"id\": \"chatcmpl-123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1677652288,\n  \"model\": \"gpt-4o-mini\",\n  \"system_fingerprint\": \"fp_44709d6fcb\",\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"\\n\\nHello there, how may I assist you today?\",\n    },\n    \"logprobs\": null,\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"completion_tokens\": 12,\n    \"total_tokens\": 21,\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0,\n      \"accepted_prediction_tokens\": 0,\n      \"rejected_prediction_tokens\": 0\n    }\n  }\n}\nThe chat completion object\nRepresents a chat completion response returned by model, based on the provided input.\n\nid\nstring\n\nA unique identifier for the chat completion.\n\nchoices\narray\n\nA list of chat completion choices. Can be more than one if n is greater than 1.\n\n\nShow properties\ncreated\ninteger\n\nThe Unix timestamp (in seconds) of when the chat completion was created.\n\nmodel\nstring\n\nThe model used for the chat completion.\n\nservice_tier\nstring or null\n\nThe service tier used for processing the request. This field is only included if the service_tier parameter is specified in the request.\n\nsystem_fingerprint\nstring\n\nThis fingerprint represents the backend configuration that the model runs with.\n\nCan be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.\n\nobject\nstring\n\nThe object type, which is always chat.completion.\n\nusage\nobject\n\nUsage statistics for the completion request.\n\n\nShow properties\nThe chat completion object\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n{\n  \"id\": \"chatcmpl-123456\",\n  \"object\": \"chat.completion\",\n  \"created\": 1728933352,\n  \"model\": \"gpt-4o-2024-08-06\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hi there! How can I assist you today?\",\n        \"refusal\": null\n      },\n      \"logprobs\": null,\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 19,\n    \"completion_tokens\": 10,\n    \"total_tokens\": 29,\n    \"prompt_tokens_details\": {\n      \"cached_tokens\": 0\n    },\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0,\n      \"accepted_prediction_tokens\": 0,\n      \"rejected_prediction_tokens\": 0\n    }\n  },\n  \"system_fingerprint\": \"fp_6b68a8204b\"\n}\nThe chat completion chunk object\nRepresents a streamed chunk of a chat completion response returned by model, based on the provided input.\n\nid\nstring\n\nA unique identifier for the chat completion. Each chunk has the same ID.\n\nchoices\narray\n\nA list of chat completion choices. Can contain more than one elements if n is greater than 1. Can also be empty for the last chunk if you set stream_options: {\"include_usage\": true}.\n\n\nShow properties\ncreated\ninteger\n\nThe Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.\n\nmodel\nstring\n\nThe model to generate the completion.\n\nservice_tier\nstring or null\n\nThe service tier used for processing the request. This field is only included if the service_tier parameter is specified in the request.\n\nsystem_fingerprint\nstring\n\nThis fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.\n\nobject\nstring\n\nThe object type, which is always chat.completion.chunk.\n\nusage\nobject or null\n\nAn optional field that will only be present when you set stream_options: {\"include_usage\": true} in your request. When present, it contains a null value except for the last chunk which contains the token usage statistics for the entire request.\n\n\nShow properties\n\nEnd of selected content from https://platform.openai.com/docs/api-reference/chat/create"
    ],
    [
      "selection:1731168187467",
      "Selected content from https://platform.openai.com/docs/api-reference/images on 2024-11-09 16:03:07\n\nImages\nGiven a prompt and/or an input image, the model will generate a new image. Related guide: Image generation\n\nCreate image\nPOST\n \nhttps://api.openai.com/v1/images/generations\nCreates an image given a prompt.\n\nRequest body\nprompt\nstring\n\nRequired\nA text description of the desired image(s). The maximum length is 1000 characters for dall-e-2 and 4000 characters for dall-e-3.\n\nmodel\nstring\n\nOptional\nDefaults to dall-e-2\nThe model to use for image generation.\n\nn\ninteger or null\n\nOptional\nDefaults to 1\nThe number of images to generate. Must be between 1 and 10. For dall-e-3, only n=1 is supported.\n\nquality\nstring\n\nOptional\nDefaults to standard\nThe quality of the image that will be generated. hd creates images with finer details and greater consistency across the image. This param is only supported for dall-e-3.\n\nresponse_format\nstring or null\n\nOptional\nDefaults to url\nThe format in which the generated images are returned. Must be one of url or b64_json. URLs are only valid for 60 minutes after the image has been generated.\n\nsize\nstring or null\n\nOptional\nDefaults to 1024x1024\nThe size of the generated images. Must be one of 256x256, 512x512, or 1024x1024 for dall-e-2. Must be one of 1024x1024, 1792x1024, or 1024x1792 for dall-e-3 models.\n\nstyle\nstring or null\n\nOptional\nDefaults to vivid\nThe style of the generated images. Must be one of vivid or natural. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This param is only supported for dall-e-3.\n\nuser\nstring\n\nOptional\nA unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.\n\nReturns\nReturns a list of image objects.\n\nExample request\nnode.js\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const image = await openai.images.generate({ model: \"dall-e-3\", prompt: \"A cute baby sea otter\" });\n\n  console.log(image.data);\n}\nmain();\nResponse\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n{\n  \"created\": 1589478378,\n  \"data\": [\n    {\n      \"url\": \"https://...\"\n    },\n    {\n      \"url\": \"https://...\"\n    }\n  ]\n}\nCreate image edit\nPOST\n \nhttps://api.openai.com/v1/images/edits\nCreates an edited or extended image given an original image and a prompt.\n\nRequest body\nimage\nfile\n\nRequired\nThe image to edit. Must be a valid PNG file, less than 4MB, and square. If mask is not provided, image must have transparency, which will be used as the mask.\n\nprompt\nstring\n\nRequired\nA text description of the desired image(s). The maximum length is 1000 characters.\n\nmask\nfile\n\nOptional\nAn additional image whose fully transparent areas (e.g. where alpha is zero) indicate where image should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions as image.\n\nmodel\nstring\n\nOptional\nDefaults to dall-e-2\nThe model to use for image generation. Only dall-e-2 is supported at this time.\n\nn\ninteger or null\n\nOptional\nDefaults to 1\nThe number of images to generate. Must be between 1 and 10.\n\nsize\nstring or null\n\nOptional\nDefaults to 1024x1024\nThe size of the generated images. Must be one of 256x256, 512x512, or 1024x1024.\n\nresponse_format\nstring or null\n\nOptional\nDefaults to url\nThe format in which the generated images are returned. Must be one of url or b64_json. URLs are only valid for 60 minutes after the image has been generated.\n\nuser\nstring\n\nOptional\nA unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.\n\nReturns\nReturns a list of image objects.\n\nExample request\nnode.js\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nimport fs from \"fs\";\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const image = await openai.images.edit({\n    image: fs.createReadStream(\"otter.png\"),\n    mask: fs.createReadStream(\"mask.png\"),\n    prompt: \"A cute baby sea otter wearing a beret\",\n  });\n\n  console.log(image.data);\n}\nmain();\nResponse\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n{\n  \"created\": 1589478378,\n  \"data\": [\n    {\n      \"url\": \"https://...\"\n    },\n    {\n      \"url\": \"https://...\"\n    }\n  ]\n}\nCreate image variation\nPOST\n \nhttps://api.openai.com/v1/images/variations\nCreates a variation of a given image.\n\nRequest body\nimage\nfile\n\nRequired\nThe image to use as the basis for the variation(s). Must be a valid PNG file, less than 4MB, and square.\n\nmodel\nstring\n\nOptional\nDefaults to dall-e-2\nThe model to use for image generation. Only dall-e-2 is supported at this time.\n\nn\ninteger or null\n\nOptional\nDefaults to 1\nThe number of images to generate. Must be between 1 and 10. For dall-e-3, only n=1 is supported.\n\nresponse_format\nstring or null\n\nOptional\nDefaults to url\nThe format in which the generated images are returned. Must be one of url or b64_json. URLs are only valid for 60 minutes after the image has been generated.\n\nsize\nstring or null\n\nOptional\nDefaults to 1024x1024\nThe size of the generated images. Must be one of 256x256, 512x512, or 1024x1024.\n\nuser\nstring\n\nOptional\nA unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.\n\nReturns\nReturns a list of image objects.\n\nExample request\nnode.js\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\nimport fs from \"fs\";\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  const image = await openai.images.createVariation({\n    image: fs.createReadStream(\"otter.png\"),\n  });\n\n  console.log(image.data);\n}\nmain();\nResponse\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n{\n  \"created\": 1589478378,\n  \"data\": [\n    {\n      \"url\": \"https://...\"\n    },\n    {\n      \"url\": \"https://...\"\n    }\n  ]\n}\nThe image object\nRepresents the url or the content of an image generated by the OpenAI API.\n\nb64_json\nstring\n\nThe base64-encoded JSON of the generated image, if response_format is b64_json.\n\nurl\nstring\n\nThe URL of the generated image, if response_format is url (default).\n\nrevised_prompt\nstring\n\nThe prompt that was used to generate the image, if there was any revision to the prompt.\n\nEnd of selected content from https://platform.openai.com/docs/api-reference/images"
    ],
    [
      "selection:1731168264202",
      "Selected content from https://platform.openai.com/docs/quickstart on 2024-11-09 16:04:24\n\nDeveloper quickstart\nLearn how to make your first API request.\nThe OpenAI API provides a simple interface to state-of-the-art AI models for natural language processing, image generation, semantic search, and speech recognition. Follow this guide to learn how to generate human-like responses to natural language prompts, create vector embeddings for semantic search, and generate images from textual descriptions.\n\nCreate and export an API key\nCreate an API key in the dashboard here, which you’ll use to securely access the API. Store the key in a safe location, like a .zshrc file or another text file on your computer. Once you’ve generated an API key, export it as an environment variable in your terminal.\n\n\nmacOS / Linux\n\nWindows\nExport an environment variable on macOS or Linux systems\n\n1\nexport OPENAI_API_KEY=\"your_api_key_here\"\nMake your first API request\nWith your OpenAI API key exported as an environment variable, you're ready to make your first API request. You can either use the REST API directly with the HTTP client of your choice, or use one of our official SDKs as shown below.\n\n\nJavaScript\n\nPython\n\ncurl\nTo use the OpenAI API in server-side JavaScript environments like Node.js, Deno, or Bun, you can use the official OpenAI SDK for TypeScript and JavaScript. Get started by installing the SDK using npm or your preferred package manager:\n\nInstall the OpenAI SDK with npm\n\n1\nnpm install openai\nWith the OpenAI SDK installed, create a file called example.mjs and copy one of the following examples into it:\n\n\nGenerate text\n\nGenerate an image\n\nCreate vector embeddings\nCreate a human-like response to a prompt\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nimport OpenAI from \"openai\";\nconst openai = new OpenAI();\n\nconst completion = await openai.chat.completions.create({\n    model: \"gpt-4o-mini\",\n    messages: [\n        { role: \"system\", content: \"You are a helpful assistant.\" },\n        {\n            role: \"user\",\n            content: \"Write a haiku about recursion in programming.\",\n        },\n    ],\n});\n\nconsole.log(completion.choices[0].message);\nExecute the code with node example.mjs (or the equivalent command for Deno or Bun). In a few moments, you should see the output of your API request!\n\nEnd of selected content from https://platform.openai.com/docs/quickstart"
    ],
    [
      "selection:1731168348570",
      "Selected content from https://platform.openai.com/docs/models#models-overview on 2024-11-09 16:05:48\n\nModels overview\nThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make customizations to our models for your specific use case with fine-tuning.\n\nMODEL\tDESCRIPTION\nGPT-4o\tOur high-intelligence flagship model for complex, multi-step tasks\nGPT-4o mini\tOur affordable and intelligent small model for fast, lightweight tasks\no1-preview and o1-mini\tLanguage models trained with reinforcement learning to perform complex reasoning.\nGPT-4 Turbo and GPT-4\tThe previous set of high-intelligence models\nGPT-3.5 Turbo\tA fast, inexpensive model for simple tasks\nDALL·E\tA model that can generate and edit images given a natural language prompt\nTTS\tA set of models that can convert text into natural sounding spoken audio\nWhisper\tA model that can convert audio into text\nEmbeddings\tA set of models that can convert text into a numerical form\nModeration\tA fine-tuned model that can detect whether text may be sensitive or unsafe\nDeprecated\tA full list of models that have been deprecated along with the suggested replacement\nFor GPT-series models, the context window refers to the maximum number of tokens that can be used in a single request, inclusive of both input and output tokens.\n\nWe have also published open source models including Point-E, Whisper, Jukebox, and CLIP.\n\nContinuous model upgrades\ngpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-4, and gpt-3.5-turbo point to their respective latest model version. You can verify this by looking at the response object after sending a request. The response will include the specific model version used (e.g. gpt-3.5-turbo-1106). The chatgpt-4o-latest model version continuously points to the version of GPT-4o used in ChatGPT, and is updated frequently, when there are significant changes. With the exception of chatgpt-4o-latest, we offer pinned model versions that developers can continue using for at least three months after an updated model has been introduced.\n\nLearn more about model deprecation on our deprecation page.\n\nGPT-4o\nGPT-4o (“o” for “omni”) is our most advanced GPT model. It is multimodal (accepting text or image inputs and outputting text), and it has the same high intelligence as GPT-4 Turbo but is much more efficient—it generates text 2x faster and is 50% cheaper. Additionally, GPT-4o has the best vision and performance across non-English languages of any of our models. GPT-4o is available in the OpenAI API to paying customers. Learn how to use GPT-4o in our text generation guide.\n\nMODEL\tCONTEXT WINDOW\tMAX OUTPUT TOKENS\tTRAINING DATA\ngpt-4o\nOur high-intelligence flagship model for complex, multi-step tasks. GPT-4o is cheaper and faster than GPT-4 Turbo. Currently points to gpt-4o-2024-08-06.\n128,000 tokens\t16,384 tokens\tUp to Oct 2023\ngpt-4o-2024-08-06\nLatest snapshot that supports Structured Outputs. gpt-4o currently points to this version.\n128,000 tokens\t16,384 tokens\tUp to Oct 2023\ngpt-4o-2024-05-13\nOriginal gpt-4o snapshot from May 13, 2024.\n128,000 tokens\t4,096 tokens\tUp to Oct 2023\nchatgpt-4o-latest\nThe chatgpt-4o-latest model version continuously points to the version of GPT-4o used in ChatGPT, and is updated frequently, when there are significant changes.\n128,000 tokens\t16,384 tokens\tUp to Oct 2023\nGPT-4o mini\nGPT-4o mini (“o” for “omni”) is our most advanced model in the small models category, and our cheapest model yet. It is multimodal (accepting text or image inputs and outputting text), has higher intelligence than gpt-3.5-turbo but is just as fast. It is meant to be used for smaller tasks, including vision tasks.\n\nWe recommend choosing gpt-4o-mini where you would have previously used gpt-3.5-turbo as this model is more capable and cheaper.\n\nMODEL\tCONTEXT WINDOW\tMAX OUTPUT TOKENS\tTRAINING DATA\ngpt-4o-mini\nOur affordable and intelligent small model for fast, lightweight tasks. GPT-4o mini is cheaper and more capable than GPT-3.5 Turbo. Currently points to gpt-4o-mini-2024-07-18.\n128,000 tokens\t16,384 tokens\tUp to Oct 2023\ngpt-4o-mini-2024-07-18\ngpt-4o-mini currently points to this version.\n128,000 tokens\t16,384 tokens\tUp to Oct 2023\nGPT-4o Realtime + Audio Beta\n\nThis is a preview release of the GPT-4o Realtime and Audio models. The gpt-4o-realtime-* models are capable of responding to audio and text inputs over a WebSocket interface. Learn more in the Realtime API guide. The gpt-4o-audio-* models below can be used in Chat Completions to generate audio responses.\n\nMODEL\tCONTEXT WINDOW\tMAX OUTPUT TOKENS\tTRAINING DATA\ngpt-4o-realtime-preview\nPreview release for the Realtime API\n128,000 tokens\t4,096 tokens\tUp to Oct 2023\ngpt-4o-realtime-preview-2024-10-01\nCurrent snapshot for the Realtime API model.\n128,000 tokens\t4,096 tokens\tUp to Oct 2023\ngpt-4o-audio-preview\nPreview release for audio inputs in chat completions.\n128,000 tokens\t16,384 tokens\tUp to Oct 2023\ngpt-4o-audio-preview-2024-10-01\nCurrent snapshot for the Audio API model.\n128,000 tokens\t16,384 tokens\tUp to Oct 2023\no1-preview and o1-mini Beta\n\nThe o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.\nLearn about the capabilities and limitations of o1 models in our reasoning guide.\n\nThere are two model types available today:\n\no1-preview: reasoning model designed to solve hard problems across domains.\no1-mini: faster and cheaper reasoning model particularly good at coding, math, and science.\nMODEL\tCONTEXT WINDOW\tMAX OUTPUT TOKENS\tTRAINING DATA\no1-preview\nPoints to the most recent snapshot of the o1 model: o1-preview-2024-09-12\n128,000 tokens\t32,768 tokens\tUp to Oct 2023\no1-preview-2024-09-12\nLatest o1 model snapshot\n128,000 tokens\t32,768 tokens\tUp to Oct 2023\no1-mini\nPoints to the most recent o1-mini snapshot: o1-mini-2024-09-12\n128,000 tokens\t65,536 tokens\tUp to Oct 2023\no1-mini-2024-09-12\nLatest o1-mini model snapshot\n128,000 tokens\t65,536 tokens\tUp to Oct 2023\nGPT-4 Turbo and GPT-4\nGPT-4 is a large multimodal model (accepting text or image inputs and outputting text) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilities. GPT-4 is available in the OpenAI API to paying customers. Like gpt-3.5-turbo, GPT-4 is optimized for chat but works well for traditional completions tasks using the Chat Completions API. Learn how to use GPT-4 in our text generation guide.\n\nMODEL\tCONTEXT WINDOW\tMAX OUTPUT TOKENS\tTRAINING DATA\ngpt-4-turbo\nThe latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling. Currently points to gpt-4-turbo-2024-04-09.\n128,000 tokens\t4,096 tokens\tUp to Dec 2023\ngpt-4-turbo-2024-04-09\nGPT-4 Turbo with Vision model. Vision requests can now use JSON mode and function calling. gpt-4-turbo currently points to this version.\n128,000 tokens\t4,096 tokens\tUp to Dec 2023\ngpt-4-turbo-preview\nGPT-4 Turbo preview model. Currently points to gpt-4-0125-preview.\n128,000 tokens\t4,096 tokens\tUp to Dec 2023\ngpt-4-0125-preview\nGPT-4 Turbo preview model intended to reduce cases of “laziness” where the model doesn’t complete a task. Learn more.\n128,000 tokens\t4,096 tokens\tUp to Dec 2023\ngpt-4-1106-preview\nGPT-4 Turbo preview model featuring improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. This is a preview model. Learn more.\n128,000 tokens\t4,096 tokens\tUp to Apr 2023\ngpt-4\nCurrently points to gpt-4-0613. See continuous model upgrades.\n8,192 tokens\t8,192 tokens\tUp to Sep 2021\ngpt-4-0613\nSnapshot of gpt-4 from June 13th 2023 with improved function calling support.\n8,192 tokens\t8,192 tokens\tUp to Sep 2021\ngpt-4-0314 Legacy\nSnapshot of gpt-4 from March 14th 2023.\n8,192 tokens\t8,192 tokens\tUp to Sep 2021\nFor many basic tasks, the difference between GPT-4 and GPT-3.5 models is not significant. However, in more complex reasoning situations, GPT-4 is much more capable than any of our previous models.\n\nMultilingual capabilities\nGPT-4 outperforms both previous large language models and as of 2023, most state-of-the-art systems (which often have benchmark-specific training or hand-engineering). On the MMLU benchmark, an English-language suite of multiple-choice questions covering 57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but also demonstrates strong performance in other languages.\n\nGPT-3.5 Turbo\nGPT-3.5 Turbo models can understand and generate natural language or code and have been optimized for chat using the Chat Completions API but work well for non-chat tasks as well.\n\nAs of July 2024, gpt-4o-mini should be used in place of gpt-3.5-turbo, as it is cheaper, more capable, multimodal, and just as fast. gpt-3.5-turbo is still available for use in the API.\n\nMODEL\tCONTEXT WINDOW\tMAX OUTPUT TOKENS\tTRAINING DATA\ngpt-3.5-turbo-0125\nThe latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls. Learn more.\n16,385 tokens\t4,096 tokens\tUp to Sep 2021\ngpt-3.5-turbo\nCurrently points to gpt-3.5-turbo-0125.\n16,385 tokens\t4,096 tokens\tUp to Sep 2021\ngpt-3.5-turbo-1106\nGPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Learn more.\n16,385 tokens\t4,096 tokens\tUp to Sep 2021\ngpt-3.5-turbo-instruct\nSimilar capabilities as GPT-3 era models. Compatible with legacy Completions endpoint and not Chat Completions.\n4,096 tokens\t4,096 tokens\tUp to Sep 2021\nDALL·E\nDALL·E is a AI system that can create realistic images and art from a description in natural language. DALL·E 3 currently supports the ability, given a prompt, to create a new image with a specific size. DALL·E 2 also support the ability to edit an existing image, or create variations of a user provided image.\n\nDALL·E 3 is available through our Images API along with DALL·E 2. You can try DALL·E 3 through ChatGPT Plus.\n\nMODEL\tDESCRIPTION\ndall-e-3\tThe latest DALL·E model released in Nov 2023. Learn more.\ndall-e-2\tThe previous DALL·E model released in Nov 2022. The 2nd iteration of DALL·E with more realistic, accurate, and 4x greater resolution images than the original model.\nTTS\nTTS is an AI model that converts text to natural sounding spoken text. We offer two different model variates, tts-1 is optimized for real time text to speech use cases and tts-1-hd is optimized for quality. These models can be used with the Speech endpoint in the Audio API.\n\nMODEL\tDESCRIPTION\ntts-1\tThe latest text to speech model, optimized for speed.\ntts-1-hd\tThe latest text to speech model, optimized for quality.\nWhisper\nWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name.\n\nCurrently, there is no difference between the open source version of Whisper and the version available through our API. However, through our API, we offer an optimized inference process which makes running Whisper through our API much faster than doing it through other means. For more technical details on Whisper, you can read the paper.\n\nEmbeddings\nEmbeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks. You can read more about our latest embedding models in the announcement blog post.\n\nMODEL\tOUTPUT DIMENSION\ntext-embedding-3-large\nMost capable embedding model for both english and non-english tasks\n3,072\ntext-embedding-3-small\nIncreased performance over 2nd generation ada embedding model\n1,536\ntext-embedding-ada-002\nMost capable 2nd generation embedding model, replacing 16 first generation models\n1,536\nModeration\nThe Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in categories like hate, self-harm, sexual content, violence, and others. Learn more about moderating text and images in our moderation guide.\n\nMODEL\tMAX TOKENS\nomni-moderation-latest\nCurrently points to omni-moderation-2024-09-26.\n32,768\nomni-moderation-2024-09-26\nLatest pinned version of our new multi-modal moderation model, capable of analyzing both text and images.\n32,768\ntext-moderation-latest\nCurrently points to text-moderation-007.\n32,768\ntext-moderation-stable\nCurrently points to text-moderation-007.\n32,768\ntext-moderation-007\nPrevious generation text-only moderation. We expect omni-moderation-* models to be the best default moving forward.\n32,768\nGPT base\nGPT base models can understand and generate natural language or code but are not trained with instruction following. These models are made to be replacements for our original GPT-3 base models and use the legacy Completions API. Most customers should use GPT-3.5 or GPT-4.\n\nMODEL\tMAX TOKENS\tTRAINING DATA\nbabbage-002\nReplacement for the GPT-3 ada and babbage base models.\n16,384 tokens\tUp to Sep 2021\ndavinci-002\nReplacement for the GPT-3 curie and davinci base models.\n16,384 tokens\tUp to Sep 2021\nHow we use your data\nYour data is your data.\n\nAs of March 1, 2023, data sent to the OpenAI API will not be used to train or improve OpenAI models (unless you explicitly opt-in to share data with us, such as by providing feedback in the Playground). One advantage to opting in is that the models may get better at your use case over time.\n\nTo help identify abuse, API data may be retained for up to 30 days, after which it will be deleted (unless otherwise required by law). For trusted customers with sensitive applications, zero data retention may be available. With zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request.\n\nNote that this data policy does not apply to OpenAI's non-API consumer services like ChatGPT or DALL·E Labs.\n\nDefault usage policies by endpoint\nENDPOINT\tDATA USED FOR TRAINING\tDEFAULT RETENTION\tELIGIBLE FOR ZERO RETENTION\n/v1/chat/completions*\tNo\t30 days\tYes, except (a) image inputs, (b) schemas provided for Structured Outputs, or (c) audio outputs. *\n/v1/assistants\tNo\t30 days **\tNo\n/v1/threads\tNo\t30 days **\tNo\n/v1/threads/messages\tNo\t30 days **\tNo\n/v1/threads/runs\tNo\t30 days **\tNo\n/v1/vector_stores\tNo\t30 days **\tNo\n/v1/threads/runs/steps\tNo\t30 days **\tNo\n/v1/images/generations\tNo\t30 days\tNo\n/v1/images/edits\tNo\t30 days\tNo\n/v1/images/variations\tNo\t30 days\tNo\n/v1/embeddings\tNo\t30 days\tYes\n/v1/audio/transcriptions\tNo\tZero data retention\t-\n/v1/audio/translations\tNo\tZero data retention\t-\n/v1/audio/speech\tNo\t30 days\tYes\n/v1/files\tNo\tUntil deleted by customer\tNo\n/v1/fine_tuning/jobs\tNo\tUntil deleted by customer\tNo\n/v1/batches\tNo\tUntil deleted by customer\tNo\n/v1/moderations\tNo\tZero data retention\t-\n/v1/completions\tNo\t30 days\tYes\n/v1/realtime (beta)\tNo\t30 days\tNo\n* Chat Completions:\n\nImage inputs via the gpt-4o, gpt-4o-mini, chatgpt-4o-latest, or gpt-4-turbo models (or previously gpt-4-vision-preview) are not eligible for zero retention.\nAudio outputs are stored for 1 hour to enable multi-turn conversations, and are not currently eligible for zero retention.\nWhen Structured Outputs is enabled, schemas provided (either as the response_format or in the function definition) are not eligible for zero retention, though the completions themselves are.\nWhen using Stored Completions via the store: true option in the API, those completions are stored for 30 days. Completions are stored in an unfiltered form after an API response, so please avoid storing completions that contain sensitive data.\n** Assistants API:\n\nObjects related to the Assistants API are deleted from our servers 30 days after you delete them via the API or the dashboard. Objects that are not deleted via the API or dashboard are retained indefinitely.\nEvaluations:\n\nEvaluation data: When you create an evaluation, the data related to that evaluation is deleted from our servers 30 days after you delete it via the dashboard. Evaluation data that is not deleted via the dashboard is retained indefinitely.\nFor details, see our API data usage policies. To learn more about zero retention, get in touch with our sales team.\n\nModel endpoint compatibility\nENDPOINT\tLATEST MODELS\n/v1/assistants\tAll GPT-4o (except chatgpt-4o-latest), GPT-4o-mini, GPT-4, and GPT-3.5 Turbo models. The retrieval tool requires gpt-4-turbo-preview (and subsequent dated model releases) or gpt-3.5-turbo-1106 (and subsequent versions).\n/v1/audio/transcriptions\twhisper-1\n/v1/audio/translations\twhisper-1\n/v1/audio/speech\ttts-1,  tts-1-hd\n/v1/chat/completions\tAll GPT-4o (except for Realtime preview), GPT-4o-mini, GPT-4, and GPT-3.5 Turbo models and their dated releases. chatgpt-4o-latest dynamic model. Fine-tuned versions of gpt-4o,  gpt-4o-mini,  gpt-4,  and gpt-3.5-turbo.\n/v1/completions (Legacy)\tgpt-3.5-turbo-instruct,  babbage-002,  davinci-002\n/v1/embeddings\ttext-embedding-3-small,  text-embedding-3-large,  text-embedding-ada-002\n/v1/fine_tuning/jobs\tgpt-4o,  gpt-4o-mini,  gpt-4,  gpt-3.5-turbo\n/v1/moderations\ttext-moderation-stable,  text-moderation-latest\n/v1/images/generations\tdall-e-2,  dall-e-3\n/v1/realtime (beta)\tgpt-4o-realtime-preview, gpt-4o-realtime-preview-2024-10-01\n\nEnd of selected content from https://platform.openai.com/docs/models#models-overview"
    ],
    [
      "selection:1731168383169",
      "Selected content from https://platform.openai.com/docs/guides/text-generation on 2024-11-09 16:06:23\n\nText generation\nLearn how to generate text from a prompt.\nOpenAI provides simple APIs to use a large language model to generate text from a prompt, as you might using ChatGPT. These models have been trained on vast quantities of data to understand multimedia inputs and natural language instructions. From these prompts, models can generate almost any kind of text response, like code, mathematical equations, structured JSON data, or human-like prose.\n\nQuickstart\nTo generate text, you can use the chat completions endpoint in the REST API, as seen in the examples below. You can either use the REST API from the HTTP client of your choice, or use one of OpenAI's official SDKs for your preferred programming language.\n\n\nGenerate prose\n\nAnalyze an image\n\nGenerate JSON data\nCreate a human-like response to a prompt\njavascript\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nimport OpenAI from \"openai\";\nconst openai = new OpenAI();\n\nconst completion = await openai.chat.completions.create({\n    model: \"gpt-4o\",\n    messages: [\n        { role: \"system\", content: \"You are a helpful assistant.\" },\n        {\n            role: \"user\",\n            content: \"Write a haiku about recursion in programming.\",\n        },\n    ],\n});\n\nconsole.log(completion.choices[0].message);\nChoosing a model\nWhen making a text generation request, the first option to configure is which model you want to generate the response. The model you choose can greatly influence the output, and impact how much each generation request costs.\n\nA large model like gpt-4o will offer a very high level of intelligence and strong performance, while having a higher cost per token.\nA small model like gpt-4o-mini offers intelligence not quite on the level of the larger model, but is faster and less expensive per token.\nA reasoning model like the o1 family of models is slower to return a result, and uses more tokens to \"think\", but is capable of advanced reasoning, coding, and multi-step planning.\nExperiment with different models in the Playground to see which one works best for your prompts! More information on choosing a model can also be found here.\n\nBuilding prompts\nThe process of crafting prompts to get the right output from a model is called prompt engineering. By giving the model precise instructions, examples, and necessary context information (like private or specialized information that wasn't included in the model's training data), you can improve the quality and accuracy of the model's output. Here, we'll get into some high-level guidance on building prompts, but you might also find the prompt engineering guide helpful.\n\nIn the chat completions API, you create prompts by providing an array of messages that contain instructions for the model. Each message can have a different role, which influences how the model might interpret the input.\n\nUser messages\nUser messages contain instructions that request a particular type of output from the model. You can think of user messages as the messages you might type in to ChatGPT as an end user.\n\nHere's an example of a user message prompt that asks the gpt-4o model to generate a haiku poem based on a prompt.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nconst response = await openai.chat.completions.create({\n  model: \"gpt-4o\",\n  messages: [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"Write a haiku about programming.\"\n        }\n      ]\n    }\n  ]\n});\nSystem messages\nMessages with the system role act as top-level instructions to the model, and typically describe what the model is supposed to do and how it should generally behave and respond.\n\nHere's an example of a system message that modifies the behavior of the model when generating a response to a user message:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\nconst response = await openai.chat.completions.create({\n  model: \"gpt-4o\",\n  messages: [\n    {\n      \"role\": \"system\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": `\n            You are a helpful assistant that answers programming questions \n            in the style of a southern belle from the southeast United States.\n          `\n        }\n      ]\n    },\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"Are semicolons optional in JavaScript?\"\n        }\n      ]\n    }\n  ]\n});\nThis prompt returns a text output in the rhetorical style requested:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nWell, sugar, that's a fine question you've got there! Now, in the world of \nJavaScript, semicolons are indeed a bit like the pearls on a necklace – you \nmight slip by without 'em, but you sure do look more polished with 'em in place. \n\nTechnically, JavaScript has this little thing called \"automatic semicolon \ninsertion\" where it kindly adds semicolons for you where it thinks they \noughta go. However, it's not always perfect, bless its heart. Sometimes, it \nmight get a tad confused and cause all sorts of unexpected behavior.\nAssistant messages\nMessages with the assistant role are presumed to have been generated by the model, perhaps in a previous generation request (see the \"Conversations\" section below). They can also be used to provide examples to the model for how it should respond to the current request - a technique known as few-shot learning.\n\nHere's an example of using an assistant message to capture the results of a previous text generation result, and making a new request based on that.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nconst response = await openai.chat.completions.create({\n  model: \"gpt-4o\",\n  messages: [\n    {\n      \"role\": \"user\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"knock knock.\" }]\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"Who's there?\" }]\n    },\n    {\n      \"role\": \"user\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"Orange.\" }]\n    }\n  ]\n});\nGiving the model additional data to use for generation\nThe message types above can also be used to provide additional information to the model which may be outside its training data. You might want to include the results of a database query, a text document, or other resources to help the model generate a relevant response. This technique is often referred to as retrieval augmented generation, or RAG. Learn more about RAG techniques here.\n\nConversations and context\nWhile each text generation request is independent and stateless (unless you are using assistants), you can still implement multi-turn conversations by providing additional messages as parameters to your text generation request. Consider the \"knock knock\" joke example shown above:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nconst response = await openai.chat.completions.create({\n  model: \"gpt-4o\",\n  messages: [\n    {\n      \"role\": \"user\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"knock knock.\" }]\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"Who's there?\" }]\n    },\n    {\n      \"role\": \"user\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"Orange.\" }]\n    }\n  ]\n});\nBy using alternating user and assistant messages, you can capture the previous state of a conversation in one request to the model.\n\nManaging context for text generation\nAs your inputs become more complex, or you include more and more turns in a conversation, you will need to consider both output token and context window limits. Model inputs and outputs are metered in tokens, which are parsed from inputs to analyze their content and intent, and assembled to render logical outputs. Models have limits on how many tokens can be used during the lifecycle of a text generation request.\n\nOutput tokens are the tokens that are generated by a model in response to a prompt. Each model supports different limits for output tokens, documented here. For example, gpt-4o-2024-08-06 can generate a maximum of 16,384 output tokens.\nA context window describes the total tokens that can be used for both input tokens and output tokens (and for some models, reasoning tokens), documented here. For example, gpt-4o-2024-08-06 has a total context window of 128k tokens.\nIf you create a very large prompt (usually by including a lot of conversation context or additional data/examples for the model), you run the risk of exceeding the allocated context window for a model, which might result in truncated outputs.\n\nYou can use the tokenizer tool (which uses the tiktoken library) to see how many tokens are present in a string of text.\n\nOptimizing model outputs\nAs you iterate on your prompts, you will be continually trying to improve accuracy, cost, and latency.\n\nGOAL\tAVAILABLE TECHNIQUES\nAccuracy\n\nEnsure the model produces accurate and useful responses to your prompts.\n\nAccurate responses require that the model has all the information it needs to generate a response, and knows how to go about creating a response (from interpreting input to formatting and styling). Often, this will require a mix of prompt engineering, RAG, and model fine-tuning.\n\nLearn about optimizing for accuracy here.\n\nCost\n\nDrive down the total cost of model usage by reducing token usage and using cheaper models when possible.\n\nTo control costs, you can try to use fewer tokens or smaller, cheaper models. Learn more about optimizing for cost here.\n\nLatency\n\nDecrease the time it takes to generate responses to your prompts.\n\nOptimzing latency is a multi-faceted process including prompt engineering, parallelism in your own code, and more. Learn more here.\n\nEnd of selected content from https://platform.openai.com/docs/guides/text-generation"
    ],
    [
      "selection:1731168392564",
      "Selected content from https://platform.openai.com/docs/guides/images on 2024-11-09 16:06:32\n\nImage generation\nLearn how to generate or manipulate images with DALL·E.\nIntroduction\nThe Images API provides three methods for interacting with images:\n\nCreating images from scratch based on a text prompt (DALL·E 3 and DALL·E 2)\nCreating edited versions of images by having the model replace some areas of a pre-existing image, based on a new text prompt (DALL·E 2 only)\nCreating variations of an existing image (DALL·E 2 only)\nThis guide covers the basics of using these three API endpoints with useful code samples. To try DALL·E 3, head to ChatGPT.\n\nUsage\nGenerations\nThe image generations endpoint allows you to create an original image given a text prompt. When using DALL·E 3, images can have a size of 1024x1024, 1024x1792 or 1792x1024 pixels.\n\nBy default, images are generated at standard quality, but when using DALL·E 3 you can set quality: \"hd\" for enhanced detail. Square, standard quality images are the fastest to generate.\n\nYou can request 1 image at a time with DALL·E 3 (request more by making parallel requests) or up to 10 images at a time using DALL·E 2 with the n parameter.\n\nGenerate an image\nnode.js\n\n\n1\n2\n3\n4\n5\n6\n7\nconst response = await openai.images.generate({\n  model: \"dall-e-3\",\n  prompt: \"a white siamese cat\",\n  n: 1,\n  size: \"1024x1024\",\n});\nimage_url = response.data[0].url;\nWhat is new with DALL·E 3\nExplore what is new with DALL·E 3 in the OpenAI Cookbook\n\nPrompting\nWith the release of DALL·E 3, the model now takes in the default prompt provided and automatically re-write it for safety reasons, and to add more detail (more detailed prompts generally result in higher quality images).\n\nWhile it is not currently possible to disable this feature, you can use prompting to get outputs closer to your requested image by adding the following to your prompt: I NEED to test how the tool works with extremely simple prompts. DO NOT add any detail, just use it AS-IS:.\n\nThe updated prompt is visible in the revised_prompt field of the data response object.\n\nExample DALL·E 3 generations\nPROMPT\tGENERATION\nA photograph of a white Siamese cat.\t\nEach image can be returned as either a URL or Base64 data, using the response_format parameter. URLs will expire after an hour.\n\nEdits (DALL·E 2 only)\nAlso known as \"inpainting\", the image edits endpoint allows you to edit or extend an image by uploading an image and mask indicating which areas should be replaced. The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the full new image, not just the erased area. This endpoint can enable experiences like DALL·E image editing in ChatGPT Plus.\n\nEdit an image\nnode.js\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nconst response = await openai.images.edit({\n  model: \"dall-e-2\",\n  image: fs.createReadStream(\"sunlit_lounge.png\"),\n  mask: fs.createReadStream(\"mask.png\"),\n  prompt: \"A sunlit indoor lounge area with a pool containing a flamingo\",\n  n: 1,\n  size: \"1024x1024\"\n});\nimage_url = response.data[0].url;\nIMAGE\tMASK\tOUTPUT\n\t\t\nPrompt: a sunlit indoor lounge area with a pool containing a flamingo\n\n\nThe uploaded image and mask must both be square PNG images less than 4MB in size, and also must have the same dimensions as each other. The non-transparent areas of the mask are not used when generating the output, so they don’t necessarily need to match the original image like the example above.\n\nVariations (DALL·E 2 only)\nThe image variations endpoint allows you to generate a variation of a given image.\n\nGenerate an image variation\nnode.js\n\n\n1\n2\n3\n4\n5\n6\n7\nconst response = await openai.images.createVariation({\n  model: \"dall-e-2\",\n  image: fs.createReadStream(\"corgi_and_cat_paw.png\"),\n  n: 1,\n  size: \"1024x1024\"\n});\nimage_url = response.data[0].url;\nIMAGE\tOUTPUT\n\t\nSimilar to the edits endpoint, the input image must be a square PNG image less than 4MB in size.\n\nContent moderation\nPrompts and images are filtered based on our content policy, returning an error when a prompt or image is flagged.\n\nLanguage-specific tips\n\nNode.js\n\nPython\nUsing in-memory image data\nThe Node.js examples in the guide above use the fs module to read image data from disk. In some cases, you may have your image data in memory instead. Here's an example API call that uses image data stored in a Node.js Buffer object:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\n// This is the Buffer object that contains your image data\nconst buffer = [your image data];\n\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nbuffer.name = \"image.png\";\n\nasync function main() {\n  const image = await openai.images.createVariation({ model: \"dall-e-2\", image: buffer, n: 1, size: \"1024x1024\" });\n  console.log(image.data);\n}\nmain();\nWorking with TypeScript\nIf you're using TypeScript, you may encounter some quirks with image file arguments. Here's an example of working around the type mismatch by explicitly casting the argument:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nimport fs from \"fs\";\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  // Cast the ReadStream to `any` to appease the TypeScript compiler\n  const image = await openai.images.createVariation({\n    image: fs.createReadStream(\"image.png\") as any,\n  });\n\n  console.log(image.data);\n}\nmain();\nAnd here's a similar example for in-memory image data:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nimport fs from \"fs\";\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\n// This is the Buffer object that contains your image data\nconst buffer: Buffer = [your image data];\n\n// Cast the buffer to `any` so that we can set the `name` property\nconst file: any = buffer;\n\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nfile.name = \"image.png\";\n\nasync function main() {\n  const image = await openai.images.createVariation({\n    file,\n    1,\n    \"1024x1024\"\n  });\n  console.log(image.data);\n}\nmain();\nError handling\nAPI requests can potentially return errors due to invalid inputs, rate limits, or other issues. These errors can be handled with a try...catch statement, and the error details can be found in either error.response or error.message:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nimport fs from \"fs\";\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n    try {\n        const image = await openai.images.createVariation({\n            image: fs.createReadStream(\"image.png\"),\n            n: 1,\n            size: \"1024x1024\",\n        });\n        console.log(image.data);\n    } catch (error) {\n        if (error.response) {\n            console.log(error.response.status);\n            console.log(error.response.data);\n        } else {\n            console.log(error.message);\n        }\n    }\n}\n\nmain();\n\nEnd of selected content from https://platform.openai.com/docs/guides/images"
    ],
    [
      "selection:1731168512635",
      "Selected content from https://platform.openai.com/docs/guides/structured-outputs on 2024-11-09 16:08:32\n\nStructured Outputs\nEnsure responses follow JSON Schema for Structured Outputs.\nTry it out\nTry it out in the Playground or generate a ready-to-use schema definition to experiment with structured outputs.\n\n\nGenerate\nIntroduction\nJSON is one of the most widely used formats in the world for applications to exchange data.\n\nStructured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied JSON Schema, so you don't need to worry about the model omitting a required key, or hallucinating an invalid enum value.\n\nSome benefits of Structed Outputs include:\n\nReliable type-safety: No need to validate or retry incorrectly formatted responses\nExplicit refusals: Safety-based model refusals are now programmatically detectable\nSimpler prompting: No need for strongly worded prompts to achieve consistent formatting\nIn addition to supporting JSON Schema in the REST API, the OpenAI SDKs for Python and JavaScript also make it easy to define object schemas using Pydantic and Zod respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.\n\nGetting a structured response\nnode.js\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\nimport OpenAI from \"openai\";\nimport { zodResponseFormat } from \"openai/helpers/zod\";\nimport { z } from \"zod\";\n\nconst openai = new OpenAI();\n\nconst CalendarEvent = z.object({\n  name: z.string(),\n  date: z.string(),\n  participants: z.array(z.string()),\n});\n\nconst completion = await openai.beta.chat.completions.parse({\n  model: \"gpt-4o-2024-08-06\",\n  messages: [\n    { role: \"system\", content: \"Extract the event information.\" },\n    { role: \"user\", content: \"Alice and Bob are going to a science fair on Friday.\" },\n  ],\n  response_format: zodResponseFormat(CalendarEvent, \"event\"),\n});\n\nconst event = completion.choices[0].message.parsed;\nSupported models\nStructured Outputs are available in our latest large language models, starting with GPT-4o:\n\ngpt-4o-mini-2024-07-18 and later\ngpt-4o-2024-08-06 and later\nOlder models like gpt-4-turbo and earlier may use JSON mode instead.\n\nWhen to use Structured Outputs via function calling vs via response_format\n\nStructured Outputs is available in two forms in the OpenAI API:\n\nWhen using function calling\nWhen using a json_schema response format\nFunction calling is useful when you are building an application that bridges the models and functionality of your application.\n\nFor example, you can give the model access to functions that query a database in order to build an AI assistant that can help users with their orders, or functions that can interact with the UI.\n\nConversely, Structured Outputs via response_format are more suitable when you want to indicate a structured schema for use when the model responds to the user, rather than when the model calls a tool.\n\nFor example, if you are building a math tutoring application, you might want the assistant to respond to your user using a specific JSON Schema so that you can generate a UI that displays different parts of the model's output in distinct ways.\n\nPut simply:\n\nIf you are connecting the model to tools, functions, data, etc. in your system, then you should use function calling\nIf you want to structure the model's output when it responds to the user, then you should use a structured response_format\nThe remainder of this guide will focus on non-function calling use cases in the Chat Completions API. To learn more about how to use Structured Outputs with function calling, check out the Function Calling guide.\n\nStructured Outputs vs JSON mode\nStructured Outputs is the evolution of JSON mode. While both ensure valid JSON is produced, only Structured Outputs ensure schema adherance. Both Structured Outputs and JSON mode are supported in the Chat Completions API, Assistants API, Fine-tuning API and Batch API.\n\nWe recommend always using Structured Outputs instead of JSON mode when possible.\n\nHowever, Structured Outputs with response_format: {type: \"json_schema\", ...} is only supported with the gpt-4o-mini, gpt-4o-mini-2024-07-18, and gpt-4o-2024-08-06 model snapshots and later.\n\nSTRUCTURED OUTPUTS\tJSON MODE\nOutputs valid JSON\tYes\tYes\nAdheres to schema\tYes (see supported schemas)\tNo\nCompatible models\tgpt-4o-mini, gpt-4o-2024-08-06, and later\tgpt-3.5-turbo, gpt-4-* and gpt-4o-* models\nEnabling\tresponse_format: { type: \"json_schema\", json_schema: {\"strict\": true, \"schema\": ...} }\tresponse_format: { type: \"json_object\" }\nExamples\n\nChain of thought\n\nStructured data extraction\n\nUI generation\n\nModeration\nChain of thought\nYou can ask the model to output an answer in a structured, step-by-step way, to guide the user through the solution.\n\nStructured Outputs for chain-of-thought math tutoring\nnode.js\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\nimport OpenAI from \"openai\";\nimport { z } from \"zod\";\nimport { zodResponseFormat } from \"openai/helpers/zod\";\n\nconst openai = new OpenAI();\n\nconst Step = z.object({\n  explanation: z.string(),\n  output: z.string(),\n});\n\nconst MathReasoning = z.object({\n  steps: z.array(Step),\n  final_answer: z.string(),\n});\n\nconst completion = await openai.beta.chat.completions.parse({\n  model: \"gpt-4o-2024-08-06\",\n  messages: [\n    { role: \"system\", content: \"You are a helpful math tutor. Guide the user through the solution step by step.\" },\n    { role: \"user\", content: \"how can I solve 8x + 7 = -23\" },\n  ],\n  response_format: zodResponseFormat(MathReasoning, \"math_reasoning\"),\n});\n\nconst math_reasoning = completion.choices[0].message.parsed;\nExample response\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n{\n  \"steps\": [\n    {\n      \"explanation\": \"Start with the equation 8x + 7 = -23.\",\n      \"output\": \"8x + 7 = -23\"\n    },\n    {\n      \"explanation\": \"Subtract 7 from both sides to isolate the term with the variable.\",\n      \"output\": \"8x = -23 - 7\"\n    },\n    {\n      \"explanation\": \"Simplify the right side of the equation.\",\n      \"output\": \"8x = -30\"\n    },\n    {\n      \"explanation\": \"Divide both sides by 8 to solve for x.\",\n      \"output\": \"x = -30 / 8\"\n    },\n    {\n      \"explanation\": \"Simplify the fraction.\",\n      \"output\": \"x = -15 / 4\"\n    }\n  ],\n  \"final_answer\": \"x = -15 / 4\"\n}\nHow to use Structured Outputs with response_format\n\nYou can use Structured Outputs with the new SDK helper to parse the model's output into your desired format, or you can specify the JSON schema directly.\n\nNote: the first request you make with any schema will have additional latency as our API processes the schema, but subsequent requests with the same schema will not have additional latency.\n\n\nSDK objects\n\nManual schema\nStep 1: Define your object\nStep 2: Supply your object in the API call\nStep 3: Handle edge cases\nStep 4: Use the generated structured data in a type-safe way\nRefusals with Structured Outputs\n\nWhen using Structured Outputs with user-generated input, OpenAI models may occasionally refuse to fulfill the request for safety reasons. Since a refusal does not necessarily follow the schema you have supplied in response_format, the API response will include a new field called refusal to indicate that the model refused to fulfill the request.\n\nWhen the refusal property appears in your output object, you might present the refusal in your UI, or include conditional logic in code that consumes the response to handle the case of a refused request.\n\nnode.js\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\nconst Step = z.object({\n  explanation: z.string(),\n  output: z.string(),\n});\n\nconst MathReasoning = z.object({\n  steps: z.array(Step),\n  final_answer: z.string(),\n});\n\nconst completion = await openai.beta.chat.completions.parse({\n  model: \"gpt-4o-2024-08-06\",\n  messages: [\n    { role: \"system\", content: \"You are a helpful math tutor. Guide the user through the solution step by step.\" },\n    { role: \"user\", content: \"how can I solve 8x + 7 = -23\" },\n  ],\n  response_format: zodResponseFormat(MathReasoning, \"math_reasoning\"),\n});\n\nconst math_reasoning = completion.choices[0].message\n\n// If the model refuses to respond, you will get a refusal message\nif (math_reasoning.refusal) {\n  console.log(math_reasoning.refusal);\n} else {\n  console.log(math_reasoning.parsed);\n}\nThe API response from a refusal will look something like this:\n\njson\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n{\n  \"id\": \"chatcmpl-9nYAG9LPNonX8DAyrkwYfemr3C8HC\",\n  \"object\": \"chat.completion\",\n  \"created\": 1721596428,\n  \"model\": \"gpt-4o-2024-08-06\",\n  \"choices\": [\n    {\n\t  \"index\": 0,\n\t  \"message\": {\n            \"role\": \"assistant\",\n            \"refusal\": \"I'm sorry, I cannot assist with that request.\"\n\t  },\n\t  \"logprobs\": null,\n\t  \"finish_reason\": \"stop\"\n\t}\n  ],\n  \"usage\": {\n      \"prompt_tokens\": 81,\n      \"completion_tokens\": 11,\n      \"total_tokens\": 92,\n      \"completion_tokens_details\": {\n        \"reasoning_tokens\": 0,\n        \"accepted_prediction_tokens\": 0,\n        \"rejected_prediction_tokens\": 0\n      }\n  },\n  \"system_fingerprint\": \"fp_3407719c7f\"\n}\nTips and best practices\n\nHandling user-generated input\nIf your application is using user-generated input, make sure your prompt includes instructions on how to handle situations where the input cannot result in a valid response.\n\nThe model will always try to adhere to the provided schema, which can result in hallucinations if the input is completely unrelated to the schema.\n\nYou could include language in your prompt to specify that you want to return empty parameters, or a specific sentence, if the model detects that the input is incompatible with the task.\n\nHandling mistakes\nStructured Outputs can still contain mistakes. If you see mistakes, try adjusting your instructions, providing examples in the system instructions, or splitting tasks into simpler subtasks. Refer to the prompt engineering guide for more guidance on how to tweak your inputs.\n\nAvoid JSON schema divergence\nTo prevent your JSON Schema and corresponding types in your programming language from diverging, we strongly recommend using the native Pydantic/zod sdk support.\n\nIf you prefer to specify the JSON schema directly, you could add CI rules that flag when either the JSON schema or underlying data objects are edited, or add a CI step that auto-generates the JSON Schema from type definitions (or vice-versa).\n\nSupported schemas\nStructured Outputs supports a subset of the JSON Schema language.\n\nSupported types\nThe following types are supported for Structured Outputs:\n\nString\nNumber\nBoolean\nInteger\nObject\nArray\nEnum\nanyOf\nRoot objects must not be anyOf\nNote that the root level object of a schema must be an object, and not use anyOf. A pattern that appears in Zod (as one example) is using a discriminated union, which produces an anyOf at the top level. So code such as the following won't work:\n\njavascript\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\nimport { z } from 'zod';\nimport { zodResponseFormat } from 'openai/helpers/zod';\n\nconst BaseResponseSchema = z.object({ /* ... */ });\nconst UnsuccessfulResponseSchema = z.object({ /* ... */ });\n\nconst finalSchema = z.discriminatedUnion('status', [\n    BaseResponseSchema,\n    UnsuccessfulResponseSchema,\n]);\n\n// Invalid JSON Schema for Structured Outputs\nconst json = zodResponseFormat(finalSchema, 'final_schema');\nAll fields must be required\nTo use Structured Outputs, all fields or function parameters must be specified as required.\n\njson\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n    \"name\": \"get_weather\",\n    \"description\": \"Fetches the weather in the given location\",\n    \"strict\": true,\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The location to get the weather for\"\n            },\n            \"unit\": {\n                \"type\": \"string\",\n                \"description\": \"The unit to return the temperature in\",\n                \"enum\": [\"F\", \"C\"]\n            }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\"location\", \"unit\"]\n    }\n}\nAlthough all fields must be required (and the model will return a value for each parameter), it is possible to emulate an optional parameter by using a union type with null.\n\njson\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n{\n    \"name\": \"get_weather\",\n    \"description\": \"Fetches the weather in the given location\",\n    \"strict\": true,\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The location to get the weather for\"\n            },\n            \"unit\": {\n                \"type\": [\"string\", \"null\"],\n                \"description\": \"The unit to return the temperature in\",\n                \"enum\": [\"F\", \"C\"]\n            }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\n            \"location\", \"unit\"\n        ]\n    }\n}\nObjects have limitations on nesting depth and size\nA schema may have up to 100 object properties total, with up to 5 levels of nesting.\n\nLimitations on total string size\nIn a schema, total string length of all property names, definition names, enum values, and const values cannot exceed 15,000 characters.\n\nLimitations on enum size\nA schema may have up to 500 enum values across all enum properties.\n\nFor a single enum property with string values, the total string length of all enum values cannot exceed 7,500 characters when there are more than 250 enum values.\n\nadditionalProperties: false must always be set in objects\nadditionalProperties controls whether it is allowable for an object to contain additional keys / values that were not defined in the JSON Schema.\n\nStructured Outputs only supports generating specified keys / values, so we require developers to set additionalProperties: false to opt into Structured Outputs.\n\njson\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n{\n    \"name\": \"get_weather\",\n    \"description\": \"Fetches the weather in the given location\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The location to get the weather for\"\n            },\n            \"unit\": {\n                \"type\": \"string\",\n                \"description\": \"The unit to return the temperature in\",\n                \"enum\": [\"F\", \"C\"]\n            }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\n            \"location\", \"unit\"\n        ]\n    }\n}\nKey ordering\nWhen using Structured Outputs, outputs will be produced in the same order as the ordering of keys in the schema.\n\nSome type-specific keywords are not yet supported\nNotable keywords not supported include:\n\nFor strings: minLength, maxLength, pattern, format\nFor numbers: minimum, maximum, multipleOf\nFor objects: patternProperties, unevaluatedProperties, propertyNames, minProperties, maxProperties\nFor arrays: unevaluatedItems, contains, minContains, maxContains, minItems, maxItems, uniqueItems\nIf you turn on Structured Outputs by supplying strict: true and call the API with an unsupported JSON Schema, you will receive an error.\n\nFor anyOf, the nested schemas must each be a valid JSON Schema per this subset\nHere's an example supported anyOf schema:\n\njson\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n{\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"item\": {\n\t\t\t\"anyOf\": [\n\t\t\t\t{\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"description\": \"The user object to insert into the database\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"name\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\"description\": \"The name of the user\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"age\": {\n\t\t\t\t\t\t\t\"type\": \"number\",\n\t\t\t\t\t\t\t\"description\": \"The age of the user\"\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\t\"additionalProperties\": false,\n\t\t\t\t\t\"required\": [\n\t\t\t\t\t\t\"name\",\n\t\t\t\t\t\t\"age\"\n\t\t\t\t\t]\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"description\": \"The address object to insert into the database\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"number\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\"description\": \"The number of the address. Eg. for 123 main st, this would be 123\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"street\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\"description\": \"The street name. Eg. for 123 main st, this would be main st\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"city\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\"description\": \"The city of the address\"\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\t\"additionalProperties\": false,\n\t\t\t\t\t\"required\": [\n\t\t\t\t\t\t\"number\",\n\t\t\t\t\t\t\"street\",\n\t\t\t\t\t\t\"city\"\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t]\n\t\t}\n\t},\n\t\"additionalProperties\": false,\n\t\"required\": [\n\t\t\"item\"\n\t]\n}\nDefinitions are supported\nYou can use definitions to define subschemas which are referenced throughout your schema. The following is a simple example.\n\njson\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n{\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"steps\": {\n\t\t\t\"type\": \"array\",\n\t\t\t\"items\": {\n\t\t\t\t\"$ref\": \"#/$defs/step\"\n\t\t\t}\n\t\t},\n\t\t\"final_answer\": {\n\t\t\t\"type\": \"string\"\n\t\t}\n\t},\n\t\"$defs\": {\n\t\t\"step\": {\n\t\t\t\"type\": \"object\",\n\t\t\t\"properties\": {\n\t\t\t\t\"explanation\": {\n\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t},\n\t\t\t\t\"output\": {\n\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"required\": [\n\t\t\t\t\"explanation\",\n\t\t\t\t\"output\"\n\t\t\t],\n\t\t\t\"additionalProperties\": false\n\t\t}\n\t},\n\t\"required\": [\n\t\t\"steps\",\n\t\t\"final_answer\"\n\t],\n\t\"additionalProperties\": false\n}\nRecursive schemas are supported\nSample recursive schema using # to indicate root recursion.\n\njson\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n{\n        \"name\": \"ui\",\n        \"description\": \"Dynamically generated UI\",\n        \"strict\": true,\n        \"schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"type\": {\n                    \"type\": \"string\",\n                    \"description\": \"The type of the UI component\",\n                    \"enum\": [\"div\", \"button\", \"header\", \"section\", \"field\", \"form\"]\n                },\n                \"label\": {\n                    \"type\": \"string\",\n                    \"description\": \"The label of the UI component, used for buttons or form fields\"\n                },\n                \"children\": {\n                    \"type\": \"array\",\n                    \"description\": \"Nested UI components\",\n                    \"items\": {\n                        \"$ref\": \"#\"\n                    }\n                },\n                \"attributes\": {\n                    \"type\": \"array\",\n                    \"description\": \"Arbitrary attributes for the UI component, suitable for any element\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"name\": {\n                                \"type\": \"string\",\n                                \"description\": \"The name of the attribute, for example onClick or className\"\n                            },\n                            \"value\": {\n                                \"type\": \"string\",\n                                \"description\": \"The value of the attribute\"\n                            }\n                        },\n                      \"additionalProperties\": false,\n                      \"required\": [\"name\", \"value\"]\n                    }\n                }\n            },\n            \"required\": [\"type\", \"label\", \"children\", \"attributes\"],\n            \"additionalProperties\": false\n        }\n    }\nSample recursive schema using explicit recursion:\n\njson\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n{\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"linked_list\": {\n\t\t\t\"$ref\": \"#/$defs/linked_list_node\"\n\t\t}\n\t},\n\t\"$defs\": {\n\t\t\"linked_list_node\": {\n\t\t\t\"type\": \"object\",\n\t\t\t\"properties\": {\n\t\t\t\t\"value\": {\n\t\t\t\t\t\"type\": \"number\"\n\t\t\t\t},\n\t\t\t\t\"next\": {\n\t\t\t\t\t\"anyOf\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"$ref\": \"#/$defs/linked_list_node\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"type\": \"null\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"additionalProperties\": false,\n\t\t\t\"required\": [\n\t\t\t\t\"next\",\n\t\t\t\t\"value\"\n\t\t\t]\n\t\t}\n\t},\n\t\"additionalProperties\": false,\n\t\"required\": [\n\t\t\"linked_list\"\n\t]\n}\nJSON mode\nJSON mode is a more basic version of the Structured Outputs feature. While JSON mode ensures that model output is valid JSON, Structured Outputs reliably matches the model's output to the schema you specify. We recommend you use Structured Outputs if it is supported for your use case.\n\nWhen JSON mode is turned on, the model's output is ensured to be valid JSON, except for in some edge cases that you should detect and handle appropriately.\n\nTo turn on JSON mode with the Chat Completions or Assistants API you can set the response_format to { \"type\": \"json_object\" }. If you are using function calling, JSON mode is always turned on.\n\nImportant notes:\n\nWhen using JSON mode, you must always instruct the model to produce JSON via some message in the conversation, for example via your system message. If you don't include an explicit instruction to generate JSON, the model may generate an unending stream of whitespace and the request may run continually until it reaches the token limit. To help ensure you don't forget, the API will throw an error if the string \"JSON\" does not appear somewhere in the context.\nJSON mode will not guarantee the output matches any specific schema, only that it is valid and parses without errors. You should use Structured Outputs to ensure it matches your schema, or if that is not possible, you should use a validation library and potentially retries to ensure that the output matches your desired schema.\nYour application must detect and handle the edge cases that can result in the model output not being a complete JSON object (see below)\nHandling edge cases\nnode.js\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\nconst we_did_not_specify_stop_tokens = true;\n\n  try {\n    const response = await openai.chat.completions.create({\n      model: \"gpt-3.5-turbo-0125\",\n      messages: [\n        {\n          role: \"system\",\n          content: \"You are a helpful assistant designed to output JSON.\",\n        },\n        { role: \"user\", content: \"Who won the world series in 2020? Please respond in the format {winner: ...}\" },\n      ],\n      response_format: { type: \"json_object\" },\n    });\n\n    // Check if the conversation was too long for the context window, resulting in incomplete JSON \n    if (response.choices[0].message.finish_reason === \"length\") {\n      // your code should handle this error case\n    }\n\n    // Check if the OpenAI safety system refused the request and generated a refusal instead\n    if (response.choices[0].message[0].refusal) {\n      // your code should handle this error case\n      // In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing\n      console.log(response.choices[0].message[0].refusal)\n    }\n\n    // Check if the model's output included restricted content, so the generation of JSON was halted and may be partial\n    if (response.choices[0].message.finish_reason === \"content_filter\") {\n      // your code should handle this error case\n    }\n\n    if (response.choices[0].message.finish_reason === \"stop\") {\n      // In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a \"stop token\"\n\n      if (we_did_not_specify_stop_tokens) {\n        // If you didn't specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object\n        // This will parse successfully and should now contain  {\"winner\": \"Los Angeles Dodgers\"}\n        console.log(JSON.parse(response.choices[0].message.content))\n      } else {\n        // Check if the response.choices[0].message.content ends with one of your stop tokens and handle appropriately\n      }\n    }\n  } catch (e) {\n    // Your code should handle errors here, for example a network error calling the API\n    console.error(e)\n  }\n\nEnd of selected content from https://platform.openai.com/docs/guides/structured-outputs"
    ]
  ],
  "selectionOrder": [
    "selection:1731167960599",
    "selection:1731168038916",
    "selection:1731168187467",
    "selection:1731168264202",
    "selection:1731168348570",
    "selection:1731168383169",
    "selection:1731168392564",
    "selection:1731168512635"
  ]
}