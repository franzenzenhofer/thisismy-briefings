{
  "selectedFiles": [],
  "selectedURLs": [],
  "selectedNotes": [
    [
      "note:1732301688416",
      "This is the ChatGPT API with Json Response Format. With working examples. Adhere to these examples and Docs for implementing ChatGPT API clients in any form. Stick to the specification and working examples."
    ]
  ],
  "selectedSpecials": [
    [
      "selection:1732300532038",
      {
        "name": "Selected Content from https://platform.openai.com/docs/api-reference/authentication",
        "icon": "✂️"
      }
    ],
    [
      "selection:1732300549398",
      {
        "name": "Selected Content from https://platform.openai.com/docs/api-reference/making-requests",
        "icon": "✂️"
      }
    ],
    [
      "selection:1732300603782",
      {
        "name": "Selected Content from https://platform.openai.com/docs/api-reference/debugging-requests",
        "icon": "✂️"
      }
    ],
    [
      "selection:1732300675692",
      {
        "name": "Selected Content from https://platform.openai.com/docs/api-reference/chat/create",
        "icon": "✂️"
      }
    ],
    [
      "selection:1732300748815",
      {
        "name": "Selected Content from https://platform.openai.com/docs/api-reference/chat/streaming",
        "icon": "✂️"
      }
    ],
    [
      "selection:1732300786911",
      {
        "name": "Selected Content from https://platform.openai.com/docs/api-reference/models",
        "icon": "✂️"
      }
    ],
    [
      "selection:1732300799153",
      {
        "name": "Selected Content from https://platform.openai.com/docs/api-reference/models/retrieve",
        "icon": "✂️"
      }
    ],
    [
      "selection:1732301022341",
      {
        "name": "Selected Content from https://platform.openai.com/docs/quickstart",
        "icon": "✂️"
      }
    ],
    [
      "selection:1732301037702",
      {
        "name": "Selected Content from https://platform.openai.com/docs/models",
        "icon": "✂️"
      }
    ],
    [
      "selection:1732301063000",
      {
        "name": "Selected Content from https://platform.openai.com/docs/guides/text-generation",
        "icon": "✂️"
      }
    ],
    [
      "selection:1732301176238",
      {
        "name": "Selected Content from https://platform.openai.com/docs/guides/structured-outputs",
        "icon": "✂️"
      }
    ],
    [
      "selection:1732301464467",
      {
        "name": "Selected Content from https://script.google.com/u/0/home/projects/1sYMfeOTTFmLFASOzxtIbMuWFDnqBqppFeu94I2ZZGj9rN9_Z-l11QhqS/edit",
        "icon": "✂️"
      }
    ]
  ],
  "outputContents": [
    [
      "selection:1732300532038",
      "Selected content from https://platform.openai.com/docs/api-reference/authentication on 2024-11-22 18:35:32\n\nAuthentication\nAPI keys\nThe OpenAI API uses API keys for authentication. You can create API keys at a user or service account level. Service accounts are tied to a \"bot\" individual and should be used to provision access for production systems. Each API key can be scoped to one of the following,\n\nProject keys - Provides access to a single project (preferred option); access Project API keys by selecting the specific project you wish to generate keys against.\nUser keys - Our legacy keys. Provides access to all organizations and all projects that user has been added to; access API Keys to view your available keys. We highly advise transitioning to project keys for best security practices, although access via this method is currently still supported.\nRemember that your API key is a secret! Do not share it with others or expose it in any client-side code (browsers, apps). Production requests must be routed through your own backend server where your API key can be securely loaded from an environment variable or key management service.\n\nAll API requests should include your API key in an Authorization HTTP header as follows:\n\n\nAuthorization: Bearer OPENAI_API_KEY\nOrganizations and projects (optional)\nFor users who belong to multiple organizations or are accessing their projects through their legacy user API key, you can pass a header to specify which organization and project is used for an API request. Usage from these API requests will count as usage for the specified organization and project.\n\nTo access the Default project in an organization, leave out the OpenAI-Project header\n\nExample curl command:\n\n\n1\n2\n3\n4\ncurl https://api.openai.com/v1/models \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"OpenAI-Organization: org-VbgeF214ETBaHgXG4rZ7iJVe\" \\\n  -H \"OpenAI-Project: $PROJECT_ID\"\nExample with the openai Python package:\n\n\n1\n2\n3\n4\n5\n6\nfrom openai import OpenAI\n\nclient = OpenAI(\n  organization='org-VbgeF214ETBaHgXG4rZ7iJVe',\n  project='$PROJECT_ID',\n)\nExample with the openai Node.js package:\n\n\n1\n2\n3\n4\n5\n6\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI({\n    organization: \"org-VbgeF214ETBaHgXG4rZ7iJVe\",\n    project: \"$PROJECT_ID\",\n});\nOrganization IDs can be found on your Organization settings page. Project IDs can be found on your General settings page by selecting the specific project.\n\nEnd of selected content from https://platform.openai.com/docs/api-reference/authentication"
    ],
    [
      "selection:1732300549398",
      "Selected content from https://platform.openai.com/docs/api-reference/making-requests on 2024-11-22 18:35:49\n\nMaking requests\nYou can paste the command below into your terminal to run your first API request. Make sure to replace $OPENAI_API_KEY with your secret API key. If you are using a legacy user key and you have multiple projects, you will also need to specify the Project Id. For improved security, we recommend transitioning to project based keys instead.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n     \"model\": \"gpt-4o-mini\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n     \"temperature\": 0.7\n   }'\nThis request queries the gpt-4o-mini model (which under the hood points to a gpt-4o-mini model variant) to complete the text starting with a prompt of \"Say this is a test\". You should get a response back that resembles the following:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n{\n    \"id\": \"chatcmpl-abc123\",\n    \"object\": \"chat.completion\",\n    \"created\": 1677858242,\n    \"model\": \"gpt-4o-mini\",\n    \"usage\": {\n        \"prompt_tokens\": 13,\n        \"completion_tokens\": 7,\n        \"total_tokens\": 20,\n        \"completion_tokens_details\": {\n            \"reasoning_tokens\": 0,\n            \"accepted_prediction_tokens\": 0,\n            \"rejected_prediction_tokens\": 0\n        }\n    },\n    \"choices\": [\n        {\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \"\\n\\nThis is a test!\"\n            },\n            \"logprobs\": null,\n            \"finish_reason\": \"stop\",\n            \"index\": 0\n        }\n    ]\n}\nNow that you've generated your first chat completion, let's break down the response object. We can see the finish_reason is stop which means the API returned the full chat completion generated by the model without running into any limits. In the choices list, we only generated a single message but you can set the n parameter to generate multiple messages choices.\n\nEnd of selected content from https://platform.openai.com/docs/api-reference/making-requests"
    ],
    [
      "selection:1732300603782",
      "Selected content from https://platform.openai.com/docs/api-reference/debugging-requests on 2024-11-22 18:36:43\n\nDebugging requests\nIn addition to error codes returned from API responses, it may sometimes be necessary to inspect HTTP response headers as well. Of particular interest will be the headers which contain the unique ID of a particular API request, and information about rate limiting applied to your requests. Below is an incomplete list of HTTP headers returned with API responses:\n\nAPI meta information\n\nopenai-organization: The organization associated with the request\nopenai-processing-ms: Time taken processing your API request\nopenai-version: REST API version used for this request (currently 2020-10-01)\nx-request-id: Unique identifier for this API request (used in troubleshooting)\nRate limiting information\n\nx-ratelimit-limit-requests\nx-ratelimit-limit-tokens\nx-ratelimit-remaining-requests\nx-ratelimit-remaining-tokens\nx-ratelimit-reset-requests\nx-ratelimit-reset-tokens\nOpenAI recommends logging request IDs in production deployments, which will allow more efficient troubleshooting with our support team should the need arise. Our official SDKs provide a property on top level response objects containing the value of the x-request-id header.\n\nRequest ID in Python\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Say this is a test\",\n    }],\n    model=\"gpt-4o-mini\",\n)\n\nprint(response._request_id)\nRequest ID in JavaScript\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nimport OpenAI from 'openai';\nconst client = new OpenAI();\n\nconst response = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n    model: 'gpt-4o-mini'\n});\n\nconsole.log(response._request_id);\n\nEnd of selected content from https://platform.openai.com/docs/api-reference/debugging-requests"
    ],
    [
      "selection:1732300675692",
      "Selected content from https://platform.openai.com/docs/api-reference/chat/create on 2024-11-22 18:37:55\n\nCreate chat completion\nPOST\n \nhttps://api.openai.com/v1/chat/completions\nCreates a model response for the given chat conversation. Learn more in the text generation, vision, and audio guides.\n\nRequest body\nmessages\narray\n\nRequired\nA list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, images, and audio.\n\n\nShow possible types\nmodel\nstring\n\nRequired\nID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API.\n\nstore\nboolean or null\n\nOptional\nDefaults to false\nWhether or not to store the output of this chat completion request for use in our model distillation or evals products.\n\nmetadata\nobject or null\n\nOptional\nDeveloper-defined tags and values used for filtering completions in the dashboard.\n\nfrequency_penalty\nnumber or null\n\nOptional\nDefaults to 0\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\nSee more information about frequency and presence penalties.\n\nlogit_bias\nmap\n\nOptional\nDefaults to null\nModify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nlogprobs\nboolean or null\n\nOptional\nDefaults to false\nWhether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.\n\ntop_logprobs\ninteger or null\n\nOptional\nAn integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n\nmax_tokens\nDeprecated\ninteger or null\n\nOptional\nThe maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.\n\nThis value is now deprecated in favor of max_completion_tokens, and is not compatible with o1 series models.\n\nmax_completion_tokens\ninteger or null\n\nOptional\nAn upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n\nn\ninteger or null\n\nOptional\nDefaults to 1\nHow many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.\n\nmodalities\narray or null\n\nOptional\nOutput types that you would like the model to generate for this request. Most models are capable of generating text, which is the default:\n\n[\"text\"]\n\nThe gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n\n[\"text\", \"audio\"]\n\nprediction\nobject\n\nOptional\nConfiguration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n\n\nShow possible types\naudio\nobject or null\n\nOptional\nParameters for audio output. Required when audio output is requested with modalities: [\"audio\"]. Learn more.\n\n\nShow properties\npresence_penalty\nnumber or null\n\nOptional\nDefaults to 0\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\nSee more information about frequency and presence penalties.\n\nresponse_format\nobject\n\nOptional\nAn object specifying the format that the model must output. Compatible with GPT-4o, GPT-4o mini, GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.\n\nSetting to { \"type\": \"json_schema\", \"json_schema\": {...} } enables Structured Outputs which ensures the model will match your supplied JSON schema. Learn more in the Structured Outputs guide.\n\nSetting to { \"type\": \"json_object\" } enables JSON mode, which ensures the message the model generates is valid JSON.\n\nImportant: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.\n\n\nShow possible types\nseed\ninteger or null\n\nOptional\nThis feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.\n\nservice_tier\nstring or null\n\nOptional\nDefaults to auto\nSpecifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:\n\nIf set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.\nIf set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.\nIf set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.\nWhen not set, the default behavior is 'auto'.\nWhen this parameter is set, the response body will include the service_tier utilized.\n\nstop\nstring / array / null\n\nOptional\nDefaults to null\nUp to 4 sequences where the API will stop generating further tokens.\n\nstream\nboolean or null\n\nOptional\nDefaults to false\nIf set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Example Python code.\n\nstream_options\nobject or null\n\nOptional\nDefaults to null\nOptions for streaming response. Only set this when you set stream: true.\n\n\nShow properties\ntemperature\nnumber or null\n\nOptional\nDefaults to 1\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or top_p but not both.\n\ntop_p\nnumber or null\n\nOptional\nDefaults to 1\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or temperature but not both.\n\ntools\narray\n\nOptional\nA list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n\n\nShow properties\ntool_choice\nstring or object\n\nOptional\nControls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n\nnone is the default when no tools are present. auto is the default if tools are present.\n\n\nShow possible types\nparallel_tool_calls\nboolean\n\nOptional\nDefaults to true\nWhether to enable parallel function calling during tool use.\n\nuser\nstring\n\nOptional\nA unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.\n\nEnd of selected content from https://platform.openai.com/docs/api-reference/chat/create"
    ],
    [
      "selection:1732300748815",
      "Selected content from https://platform.openai.com/docs/api-reference/chat/streaming on 2024-11-22 18:39:08\n\nThe chat completion object\nRepresents a chat completion response returned by model, based on the provided input.\n\nid\nstring\n\nA unique identifier for the chat completion.\n\nchoices\narray\n\nA list of chat completion choices. Can be more than one if n is greater than 1.\n\n\nShow properties\ncreated\ninteger\n\nThe Unix timestamp (in seconds) of when the chat completion was created.\n\nmodel\nstring\n\nThe model used for the chat completion.\n\nservice_tier\nstring or null\n\nThe service tier used for processing the request. This field is only included if the service_tier parameter is specified in the request.\n\nsystem_fingerprint\nstring\n\nThis fingerprint represents the backend configuration that the model runs with.\n\nCan be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.\n\nobject\nstring\n\nThe object type, which is always chat.completion.\n\nusage\nobject\n\nUsage statistics for the completion request.\n\n\nShow properties\nOBJECT The chat completion object\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n{\n  \"id\": \"chatcmpl-123456\",\n  \"object\": \"chat.completion\",\n  \"created\": 1728933352,\n  \"model\": \"gpt-4o-2024-08-06\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hi there! How can I assist you today?\",\n        \"refusal\": null\n      },\n      \"logprobs\": null,\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 19,\n    \"completion_tokens\": 10,\n    \"total_tokens\": 29,\n    \"prompt_tokens_details\": {\n      \"cached_tokens\": 0\n    },\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0,\n      \"accepted_prediction_tokens\": 0,\n      \"rejected_prediction_tokens\": 0\n    }\n  },\n  \"system_fingerprint\": \"fp_6b68a8204b\"\n}\nThe chat completion chunk object\nRepresents a streamed chunk of a chat completion response returned by model, based on the provided input.\n\nid\nstring\n\nA unique identifier for the chat completion. Each chunk has the same ID.\n\nchoices\narray\n\nA list of chat completion choices. Can contain more than one elements if n is greater than 1. Can also be empty for the last chunk if you set stream_options: {\"include_usage\": true}.\n\n\nShow properties\ncreated\ninteger\n\nThe Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.\n\nmodel\nstring\n\nThe model to generate the completion.\n\nservice_tier\nstring or null\n\nThe service tier used for processing the request. This field is only included if the service_tier parameter is specified in the request.\n\nsystem_fingerprint\nstring\n\nThis fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.\n\nobject\nstring\n\nThe object type, which is always chat.completion.chunk.\n\nusage\nobject or null\n\nAn optional field that will only be present when you set stream_options: {\"include_usage\": true} in your request. When present, it contains a null value except for the last chunk which contains the token usage statistics for the entire request.\n\n\nShow properties\n\nEnd of selected content from https://platform.openai.com/docs/api-reference/chat/streaming"
    ],
    [
      "selection:1732300786911",
      "Selected content from https://platform.openai.com/docs/api-reference/models on 2024-11-22 18:39:46\n\nModels\nList and describe the various models available in the API. You can refer to the Models documentation to understand what models are available and the differences between them.\n\nList models\nGET\n \nhttps://api.openai.com/v1/models\nLists the currently available models, and provides basic information about each one such as the owner and availability.\n\nReturns\nA list of model objects.\n\nEnd of selected content from https://platform.openai.com/docs/api-reference/models"
    ],
    [
      "selection:1732300799153",
      "Selected content from https://platform.openai.com/docs/api-reference/models/retrieve on 2024-11-22 18:39:59\n\nRetrieve model\nGET\n \nhttps://api.openai.com/v1/models/{model}\nRetrieves a model instance, providing basic information about the model such as the owner and permissioning.\n\nPath parameters\nmodel\nstring\n\nRequired\nThe ID of the model to use for this request\n\nReturns\nThe model object matching the specified ID.\n\nEnd of selected content from https://platform.openai.com/docs/api-reference/models/retrieve"
    ],
    [
      "selection:1732301022341",
      "Selected content from https://platform.openai.com/docs/quickstart on 2024-11-22 18:43:42\n\nDeveloper quickstart\nLearn how to make your first API request.\nThe OpenAI API provides a simple interface to state-of-the-art AI models for natural language processing, image generation, semantic search, and speech recognition. Follow this guide to learn how to generate human-like responses to natural language prompts, create vector embeddings for semantic search, and generate images from textual descriptions.\n\nCreate and export an API key\nCreate an API key in the dashboard here, which you’ll use to securely access the API. Store the key in a safe location, like a .zshrc file or another text file on your computer. Once you’ve generated an API key, export it as an environment variable in your terminal.\n\n\nmacOS / Linux\n\nWindows\nExport an environment variable on macOS or Linux systems\n\n1\nexport OPENAI_API_KEY=\"your_api_key_here\"\nMake your first API request\nWith your OpenAI API key exported as an environment variable, you're ready to make your first API request. You can either use the REST API directly with the HTTP client of your choice, or use one of our official SDKs as shown below.\n\n\nJavaScript\n\nPython\n\ncurl\nTo use the OpenAI API in server-side JavaScript environments like Node.js, Deno, or Bun, you can use the official OpenAI SDK for TypeScript and JavaScript. Get started by installing the SDK using npm or your preferred package manager:\n\nInstall the OpenAI SDK with npm\n\n1\nnpm install openai\nWith the OpenAI SDK installed, create a file called example.mjs and copy one of the following examples into it:\n\n\nGenerate text\n\nGenerate an image\n\nCreate vector embeddings\nCreate a human-like response to a prompt\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nimport OpenAI from \"openai\";\nconst openai = new OpenAI();\n\nconst completion = await openai.chat.completions.create({\n    model: \"gpt-4o-mini\",\n    messages: [\n        { role: \"system\", content: \"You are a helpful assistant.\" },\n        {\n            role: \"user\",\n            content: \"Write a haiku about recursion in programming.\",\n        },\n    ],\n});\n\nconsole.log(completion.choices[0].message);\nExecute the code with node example.mjs (or the equivalent command for Deno or Bun). In a few moments, you should see the output of your API request!\n\nEnd of selected content from https://platform.openai.com/docs/quickstart"
    ],
    [
      "selection:1732301037702",
      "Selected content from https://platform.openai.com/docs/models on 2024-11-22 18:43:57\n\nModels overview\nThe OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make customizations to our models for your specific use case with fine-tuning.\n\nMODEL\tDESCRIPTION\nGPT-4o\tOur high-intelligence flagship model for complex, multi-step tasks\nGPT-4o mini\tOur affordable and intelligent small model for fast, lightweight tasks\no1-preview and o1-mini\tLanguage models trained with reinforcement learning to perform complex reasoning.\nGPT-4 Turbo and GPT-4\tThe previous set of high-intelligence models\nGPT-3.5 Turbo\tA fast, inexpensive model for simple tasks\n\nEnd of selected content from https://platform.openai.com/docs/models"
    ],
    [
      "selection:1732301063000",
      "Selected content from https://platform.openai.com/docs/guides/text-generation on 2024-11-22 18:44:23\n\nText generation\nLearn how to generate text from a prompt.\nOpenAI provides simple APIs to use a large language model to generate text from a prompt, as you might using ChatGPT. These models have been trained on vast quantities of data to understand multimedia inputs and natural language instructions. From these prompts, models can generate almost any kind of text response, like code, mathematical equations, structured JSON data, or human-like prose.\n\nQuickstart\nTo generate text, you can use the chat completions endpoint in the REST API, as seen in the examples below. You can either use the REST API from the HTTP client of your choice, or use one of OpenAI's official SDKs for your preferred programming language.\n\n\nGenerate prose\n\nAnalyze an image\n\nGenerate JSON data\nCreate a human-like response to a prompt\n\njavascript\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nimport OpenAI from \"openai\";\nconst openai = new OpenAI();\n\nconst completion = await openai.chat.completions.create({\n    model: \"gpt-4o\",\n    messages: [\n        { role: \"system\", content: \"You are a helpful assistant.\" },\n        {\n            role: \"user\",\n            content: \"Write a haiku about recursion in programming.\",\n        },\n    ],\n});\n\nconsole.log(completion.choices[0].message);\nChoosing a model\nWhen making a text generation request, the first option to configure is which model you want to generate the response. The model you choose can greatly influence the output, and impact how much each generation request costs.\n\nA large model like gpt-4o will offer a very high level of intelligence and strong performance, while having a higher cost per token.\nA small model like gpt-4o-mini offers intelligence not quite on the level of the larger model, but is faster and less expensive per token.\nA reasoning model like the o1 family of models is slower to return a result, and uses more tokens to \"think\", but is capable of advanced reasoning, coding, and multi-step planning.\nExperiment with different models in the Playground to see which one works best for your prompts! More information on choosing a model can also be found here.\n\nBuilding prompts\nThe process of crafting prompts to get the right output from a model is called prompt engineering. By giving the model precise instructions, examples, and necessary context information (like private or specialized information that wasn't included in the model's training data), you can improve the quality and accuracy of the model's output. Here, we'll get into some high-level guidance on building prompts, but you might also find the prompt engineering guide helpful.\n\nIn the chat completions API, you create prompts by providing an array of messages that contain instructions for the model. Each message can have a different role, which influences how the model might interpret the input.\n\nUser messages\nUser messages contain instructions that request a particular type of output from the model. You can think of user messages as the messages you might type in to ChatGPT as an end user.\n\nHere's an example of a user message prompt that asks the gpt-4o model to generate a haiku poem based on a prompt.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nconst response = await openai.chat.completions.create({\n  model: \"gpt-4o\",\n  messages: [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"Write a haiku about programming.\"\n        }\n      ]\n    }\n  ]\n});\nSystem messages\nMessages with the system role act as top-level instructions to the model, and typically describe what the model is supposed to do and how it should generally behave and respond.\n\nHere's an example of a system message that modifies the behavior of the model when generating a response to a user message:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\nconst response = await openai.chat.completions.create({\n  model: \"gpt-4o\",\n  messages: [\n    {\n      \"role\": \"system\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": `\n            You are a helpful assistant that answers programming questions \n            in the style of a southern belle from the southeast United States.\n          `\n        }\n      ]\n    },\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"Are semicolons optional in JavaScript?\"\n        }\n      ]\n    }\n  ]\n});\nThis prompt returns a text output in the rhetorical style requested:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nWell, sugar, that's a fine question you've got there! Now, in the world of \nJavaScript, semicolons are indeed a bit like the pearls on a necklace – you \nmight slip by without 'em, but you sure do look more polished with 'em in place. \n\nTechnically, JavaScript has this little thing called \"automatic semicolon \ninsertion\" where it kindly adds semicolons for you where it thinks they \noughta go. However, it's not always perfect, bless its heart. Sometimes, it \nmight get a tad confused and cause all sorts of unexpected behavior.\nAssistant messages\nMessages with the assistant role are presumed to have been generated by the model, perhaps in a previous generation request (see the \"Conversations\" section below). They can also be used to provide examples to the model for how it should respond to the current request - a technique known as few-shot learning.\n\nHere's an example of using an assistant message to capture the results of a previous text generation result, and making a new request based on that.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nconst response = await openai.chat.completions.create({\n  model: \"gpt-4o\",\n  messages: [\n    {\n      \"role\": \"user\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"knock knock.\" }]\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"Who's there?\" }]\n    },\n    {\n      \"role\": \"user\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"Orange.\" }]\n    }\n  ]\n});\nGiving the model additional data to use for generation\nThe message types above can also be used to provide additional information to the model which may be outside its training data. You might want to include the results of a database query, a text document, or other resources to help the model generate a relevant response. This technique is often referred to as retrieval augmented generation, or RAG. Learn more about RAG techniques here.\n\nConversations and context\nWhile each text generation request is independent and stateless (unless you are using assistants), you can still implement multi-turn conversations by providing additional messages as parameters to your text generation request. Consider the \"knock knock\" joke example shown above:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nconst response = await openai.chat.completions.create({\n  model: \"gpt-4o\",\n  messages: [\n    {\n      \"role\": \"user\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"knock knock.\" }]\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"Who's there?\" }]\n    },\n    {\n      \"role\": \"user\",\n      \"content\": [{ \"type\": \"text\", \"text\": \"Orange.\" }]\n    }\n  ]\n});\nBy using alternating user and assistant messages, you can capture the previous state of a conversation in one request to the model.\n\nManaging context for text generation\nAs your inputs become more complex, or you include more and more turns in a conversation, you will need to consider both output token and context window limits. Model inputs and outputs are metered in tokens, which are parsed from inputs to analyze their content and intent, and assembled to render logical outputs. Models have limits on how many tokens can be used during the lifecycle of a text generation request.\n\nOutput tokens are the tokens that are generated by a model in response to a prompt. Each model supports different limits for output tokens, documented here. For example, gpt-4o-2024-08-06 can generate a maximum of 16,384 output tokens.\nA context window describes the total tokens that can be used for both input tokens and output tokens (and for some models, reasoning tokens), documented here. For example, gpt-4o-2024-08-06 has a total context window of 128k tokens.\nIf you create a very large prompt (usually by including a lot of conversation context or additional data/examples for the model), you run the risk of exceeding the allocated context window for a model, which might result in truncated outputs.\n\nYou can use the tokenizer tool (which uses the tiktoken library) to see how many tokens are present in a string of text.\n\nOptimizing model outputs\nAs you iterate on your prompts, you will be continually trying to improve accuracy, cost, and latency.\n\nGOAL\tAVAILABLE TECHNIQUES\nAccuracy\n\nEnsure the model produces accurate and useful responses to your prompts.\n\nAccurate responses require that the model has all the information it needs to generate a response, and knows how to go about creating a response (from interpreting input to formatting and styling). Often, this will require a mix of prompt engineering, RAG, and model fine-tuning.\n\nLearn about optimizing for accuracy here.\n\nCost\n\nDrive down the total cost of model usage by reducing token usage and using cheaper models when possible.\n\nTo control costs, you can try to use fewer tokens or smaller, cheaper models. Learn more about optimizing for cost here.\n\nLatency\n\nDecrease the time it takes to generate responses to your prompts.\n\nOptimizing latency is a multi-faceted process including prompt engineering, parallelism in your own code, and more. Learn more here.\n\nEnd of selected content from https://platform.openai.com/docs/guides/text-generation"
    ],
    [
      "selection:1732301176238",
      "Selected content from https://platform.openai.com/docs/guides/structured-outputs on 2024-11-22 18:46:16\n\nStructured Outputs\nEnsure responses follow JSON Schema for Structured Outputs.\nTry it out\nTry it out in the Playground or generate a ready-to-use schema definition to experiment with structured outputs.\n\n\nGenerate\nIntroduction\nJSON is one of the most widely used formats in the world for applications to exchange data.\n\nStructured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied JSON Schema, so you don't need to worry about the model omitting a required key, or hallucinating an invalid enum value.\n\nSome benefits of Structed Outputs include:\n\nReliable type-safety: No need to validate or retry incorrectly formatted responses\nExplicit refusals: Safety-based model refusals are now programmatically detectable\nSimpler prompting: No need for strongly worded prompts to achieve consistent formatting\nIn addition to supporting JSON Schema in the REST API, the OpenAI SDKs for Python and JavaScript also make it easy to define object schemas using Pydantic and Zod respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.\n\nGetting a structured response\n\npython\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\nSupported models\nStructured Outputs are available in our latest large language models, starting with GPT-4o:\n\ngpt-4o-mini-2024-07-18 and later\ngpt-4o-2024-08-06 and later\nOlder models like gpt-4-turbo and earlier may use JSON mode instead.\n\nWhen to use Structured Outputs via function calling vs via response_format\n\nStructured Outputs is available in two forms in the OpenAI API:\n\nWhen using function calling\nWhen using a json_schema response format\nFunction calling is useful when you are building an application that bridges the models and functionality of your application.\n\nFor example, you can give the model access to functions that query a database in order to build an AI assistant that can help users with their orders, or functions that can interact with the UI.\n\nConversely, Structured Outputs via response_format are more suitable when you want to indicate a structured schema for use when the model responds to the user, rather than when the model calls a tool.\n\nFor example, if you are building a math tutoring application, you might want the assistant to respond to your user using a specific JSON Schema so that you can generate a UI that displays different parts of the model's output in distinct ways.\n\nPut simply:\n\nIf you are connecting the model to tools, functions, data, etc. in your system, then you should use function calling\nIf you want to structure the model's output when it responds to the user, then you should use a structured response_format\nThe remainder of this guide will focus on non-function calling use cases in the Chat Completions API. To learn more about how to use Structured Outputs with function calling, check out the Function Calling guide.\n\nStructured Outputs vs JSON mode\nStructured Outputs is the evolution of JSON mode. While both ensure valid JSON is produced, only Structured Outputs ensure schema adherance. Both Structured Outputs and JSON mode are supported in the Chat Completions API, Assistants API, Fine-tuning API and Batch API.\n\nWe recommend always using Structured Outputs instead of JSON mode when possible.\n\nHowever, Structured Outputs with response_format: {type: \"json_schema\", ...} is only supported with the gpt-4o-mini, gpt-4o-mini-2024-07-18, and gpt-4o-2024-08-06 model snapshots and later.\n\nSTRUCTURED OUTPUTS\tJSON MODE\nOutputs valid JSON\tYes\tYes\nAdheres to schema\tYes (see supported schemas)\tNo\nCompatible models\tgpt-4o-mini, gpt-4o-2024-08-06, and later\tgpt-3.5-turbo, gpt-4-* and gpt-4o-* models\nEnabling\tresponse_format: { type: \"json_schema\", json_schema: {\"strict\": true, \"schema\": ...} }\tresponse_format: { type: \"json_object\" }\nExamples\n\nChain of thought\n\nStructured data extraction\n\nUI generation\n\nModeration\nChain of thought\nYou can ask the model to output an answer in a structured, step-by-step way, to guide the user through the solution.\n\nStructured Outputs for chain-of-thought math tutoring\n\npython\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\nclass MathReasoning(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n    ],\n    response_format=MathReasoning,\n)\n\nmath_reasoning = completion.choices[0].message.parsed\nExample response\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n{\n  \"steps\": [\n    {\n      \"explanation\": \"Start with the equation 8x + 7 = -23.\",\n      \"output\": \"8x + 7 = -23\"\n    },\n    {\n      \"explanation\": \"Subtract 7 from both sides to isolate the term with the variable.\",\n      \"output\": \"8x = -23 - 7\"\n    },\n    {\n      \"explanation\": \"Simplify the right side of the equation.\",\n      \"output\": \"8x = -30\"\n    },\n    {\n      \"explanation\": \"Divide both sides by 8 to solve for x.\",\n      \"output\": \"x = -30 / 8\"\n    },\n    {\n      \"explanation\": \"Simplify the fraction.\",\n      \"output\": \"x = -15 / 4\"\n    }\n  ],\n  \"final_answer\": \"x = -15 / 4\"\n}\nHow to use Structured Outputs with response_format\n\nYou can use Structured Outputs with the new SDK helper to parse the model's output into your desired format, or you can specify the JSON schema directly.\n\nNote: the first request you make with any schema will have additional latency as our API processes the schema, but subsequent requests with the same schema will not have additional latency.\n\n\nSDK objects\n\nManual schema\nStep 1: Define your object\nFirst you must define an object or data structure to represent the JSON Schema that the model should be constrained to follow. See the examples at the top of this guide for reference.\n\nWhile Structured Outputs supports much of JSON Schema, some features are unavailable either for performance or technical reasons. See here for more details.\n\nFor example, you can define an object like this:\n\n\npython\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nfrom pydantic import BaseModel\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\nclass MathResponse(BaseModel):\n    steps: list[Step]\n    final_answer: str\nTips for your data structure\nTo maximize the quality of model generations, we recommend the following:\n\nName keys clearly and intuitively\nCreate clear titles and descriptions for important keys in your structure\nCreate and use evals to determine the structure that works best for your use case\nStep 2: Supply your object in the API call\nYou can use the parse method to automatically parse the JSON response into the object you defined.\n\nUnder the hood, the SDK takes care of supplying the JSON schema corresponding to your data structure, and then parsing the response as an object.\n\n\npython\n\n1\n2\n3\n4\n5\n6\n7\n8\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n    ],\n    response_format=MathResponse\n  )\nStep 3: Handle edge cases\nIn some cases, the model might not generate a valid response that matches the provided JSON schema.\n\nThis can happen in the case of a refusal, if the model refuses to answer for safety reasons, or if for example you reach a max tokens limit and the response is incomplete.\n\n\npython\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\ntry:\n    completion = client.beta.chat.completions.parse(\n        model=\"gpt-4o-2024-08-06\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n            {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n        ],\n        response_format=MathResponse,\n        max_tokens=50\n    )\n    math_response = completion.choices[0].message\n    if math_response.parsed:\n        print(math_response.parsed)\n    elif math_response.refusal:\n        # handle refusal\n        print(math_response.refusal)\nexcept Exception as e:\n    # Handle edge cases\n    if type(e) == openai.LengthFinishReasonError:\n        # Retry with a higher max tokens\n        print(\"Too many tokens: \", e)\n        pass\n    else:\n        # Handle other exceptions\n        print(e)\n        pass\nStep 4: Use the generated structured data in a type-safe way\nWhen using the SDK, you can use the parsed attribute to access the parsed JSON response as an object. This object will be of the type you defined in the response_format parameter.\n\n\npython\n\n1\n2\n3\nmath_response = completion.choices[0].message.parsed\nprint(math_response.steps)\nprint(math_response.final_answer)\nRefusals with Structured Outputs\n\nWhen using Structured Outputs with user-generated input, OpenAI models may occasionally refuse to fulfill the request for safety reasons. Since a refusal does not necessarily follow the schema you have supplied in response_format, the API response will include a new field called refusal to indicate that the model refused to fulfill the request.\n\nWhen the refusal property appears in your output object, you might present the refusal in your UI, or include conditional logic in code that consumes the response to handle the case of a refused request.\n\n\npython\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\nclass MathReasoning(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n    ],\n    response_format=MathReasoning,\n)\n\nmath_reasoning = completion.choices[0].message\n\n# If the model refuses to respond, you will get a refusal message\nif (math_reasoning.refusal):\n    print(math_reasoning.refusal)\nelse:\n    print(math_reasoning.parsed)\nThe API response from a refusal will look something like this:\n\n\njson\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n{\n  \"id\": \"chatcmpl-9nYAG9LPNonX8DAyrkwYfemr3C8HC\",\n  \"object\": \"chat.completion\",\n  \"created\": 1721596428,\n  \"model\": \"gpt-4o-2024-08-06\",\n  \"choices\": [\n    {\n\t  \"index\": 0,\n\t  \"message\": {\n            \"role\": \"assistant\",\n            \"refusal\": \"I'm sorry, I cannot assist with that request.\"\n\t  },\n\t  \"logprobs\": null,\n\t  \"finish_reason\": \"stop\"\n\t}\n  ],\n  \"usage\": {\n      \"prompt_tokens\": 81,\n      \"completion_tokens\": 11,\n      \"total_tokens\": 92,\n      \"completion_tokens_details\": {\n        \"reasoning_tokens\": 0,\n        \"accepted_prediction_tokens\": 0,\n        \"rejected_prediction_tokens\": 0\n      }\n  },\n  \"system_fingerprint\": \"fp_3407719c7f\"\n}\nTips and best practices\n\nHandling user-generated input\nIf your application is using user-generated input, make sure your prompt includes instructions on how to handle situations where the input cannot result in a valid response.\n\nThe model will always try to adhere to the provided schema, which can result in hallucinations if the input is completely unrelated to the schema.\n\nYou could include language in your prompt to specify that you want to return empty parameters, or a specific sentence, if the model detects that the input is incompatible with the task.\n\nHandling mistakes\nStructured Outputs can still contain mistakes. If you see mistakes, try adjusting your instructions, providing examples in the system instructions, or splitting tasks into simpler subtasks. Refer to the prompt engineering guide for more guidance on how to tweak your inputs.\n\nAvoid JSON schema divergence\nTo prevent your JSON Schema and corresponding types in your programming language from diverging, we strongly recommend using the native Pydantic/zod sdk support.\n\nIf you prefer to specify the JSON schema directly, you could add CI rules that flag when either the JSON schema or underlying data objects are edited, or add a CI step that auto-generates the JSON Schema from type definitions (or vice-versa).\n\nSupported schemas\nStructured Outputs supports a subset of the JSON Schema language.\n\nSupported types\nThe following types are supported for Structured Outputs:\n\nString\nNumber\nBoolean\nInteger\nObject\nArray\nEnum\nanyOf\nRoot objects must not be anyOf\nNote that the root level object of a schema must be an object, and not use anyOf. A pattern that appears in Zod (as one example) is using a discriminated union, which produces an anyOf at the top level. So code such as the following won't work:\n\n\njavascript\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\nimport { z } from 'zod';\nimport { zodResponseFormat } from 'openai/helpers/zod';\n\nconst BaseResponseSchema = z.object({ /* ... */ });\nconst UnsuccessfulResponseSchema = z.object({ /* ... */ });\n\nconst finalSchema = z.discriminatedUnion('status', [\n    BaseResponseSchema,\n    UnsuccessfulResponseSchema,\n]);\n\n// Invalid JSON Schema for Structured Outputs\nconst json = zodResponseFormat(finalSchema, 'final_schema');\nAll fields must be required\nTo use Structured Outputs, all fields or function parameters must be specified as required.\n\n\njson\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n{\n    \"name\": \"get_weather\",\n    \"description\": \"Fetches the weather in the given location\",\n    \"strict\": true,\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The location to get the weather for\"\n            },\n            \"unit\": {\n                \"type\": \"string\",\n                \"description\": \"The unit to return the temperature in\",\n                \"enum\": [\"F\", \"C\"]\n            }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\"location\", \"unit\"]\n    }\n}\nAlthough all fields must be required (and the model will return a value for each parameter), it is possible to emulate an optional parameter by using a union type with null.\n\n\njson\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n{\n    \"name\": \"get_weather\",\n    \"description\": \"Fetches the weather in the given location\",\n    \"strict\": true,\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The location to get the weather for\"\n            },\n            \"unit\": {\n                \"type\": [\"string\", \"null\"],\n                \"description\": \"The unit to return the temperature in\",\n                \"enum\": [\"F\", \"C\"]\n            }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\n            \"location\", \"unit\"\n        ]\n    }\n}\nObjects have limitations on nesting depth and size\nA schema may have up to 100 object properties total, with up to 5 levels of nesting.\n\nLimitations on total string size\nIn a schema, total string length of all property names, definition names, enum values, and const values cannot exceed 15,000 characters.\n\nLimitations on enum size\nA schema may have up to 500 enum values across all enum properties.\n\nFor a single enum property with string values, the total string length of all enum values cannot exceed 7,500 characters when there are more than 250 enum values.\n\nadditionalProperties: false must always be set in objects\nadditionalProperties controls whether it is allowable for an object to contain additional keys / values that were not defined in the JSON Schema.\n\nStructured Outputs only supports generating specified keys / values, so we require developers to set additionalProperties: false to opt into Structured Outputs.\n\n\njson\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n{\n    \"name\": \"get_weather\",\n    \"description\": \"Fetches the weather in the given location\",\n    \"strict\": true,\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The location to get the weather for\"\n            },\n            \"unit\": {\n                \"type\": \"string\",\n                \"description\": \"The unit to return the temperature in\",\n                \"enum\": [\"F\", \"C\"]\n            }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\n            \"location\", \"unit\"\n        ]\n    }\n}\nKey ordering\nWhen using Structured Outputs, outputs will be produced in the same order as the ordering of keys in the schema.\n\nSome type-specific keywords are not yet supported\nNotable keywords not supported include:\n\nFor strings: minLength, maxLength, pattern, format\nFor numbers: minimum, maximum, multipleOf\nFor objects: patternProperties, unevaluatedProperties, propertyNames, minProperties, maxProperties\nFor arrays: unevaluatedItems, contains, minContains, maxContains, minItems, maxItems, uniqueItems\nIf you turn on Structured Outputs by supplying strict: true and call the API with an unsupported JSON Schema, you will receive an error.\n\nFor anyOf, the nested schemas must each be a valid JSON Schema per this subset\nHere's an example supported anyOf schema:\n\n\njson\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n{\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"item\": {\n\t\t\t\"anyOf\": [\n\t\t\t\t{\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"description\": \"The user object to insert into the database\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"name\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\"description\": \"The name of the user\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"age\": {\n\t\t\t\t\t\t\t\"type\": \"number\",\n\t\t\t\t\t\t\t\"description\": \"The age of the user\"\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\t\"additionalProperties\": false,\n\t\t\t\t\t\"required\": [\n\t\t\t\t\t\t\"name\",\n\t\t\t\t\t\t\"age\"\n\t\t\t\t\t]\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"description\": \"The address object to insert into the database\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"number\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\"description\": \"The number of the address. Eg. for 123 main st, this would be 123\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"street\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\"description\": \"The street name. Eg. for 123 main st, this would be main st\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"city\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\"description\": \"The city of the address\"\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\t\"additionalProperties\": false,\n\t\t\t\t\t\"required\": [\n\t\t\t\t\t\t\"number\",\n\t\t\t\t\t\t\"street\",\n\t\t\t\t\t\t\"city\"\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t]\n\t\t}\n\t},\n\t\"additionalProperties\": false,\n\t\"required\": [\n\t\t\"item\"\n\t]\n}\nDefinitions are supported\nYou can use definitions to define subschemas which are referenced throughout your schema. The following is a simple example.\n\n\njson\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n{\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"steps\": {\n\t\t\t\"type\": \"array\",\n\t\t\t\"items\": {\n\t\t\t\t\"$ref\": \"#/$defs/step\"\n\t\t\t}\n\t\t},\n\t\t\"final_answer\": {\n\t\t\t\"type\": \"string\"\n\t\t}\n\t},\n\t\"$defs\": {\n\t\t\"step\": {\n\t\t\t\"type\": \"object\",\n\t\t\t\"properties\": {\n\t\t\t\t\"explanation\": {\n\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t},\n\t\t\t\t\"output\": {\n\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"required\": [\n\t\t\t\t\"explanation\",\n\t\t\t\t\"output\"\n\t\t\t],\n\t\t\t\"additionalProperties\": false\n\t\t}\n\t},\n\t\"required\": [\n\t\t\"steps\",\n\t\t\"final_answer\"\n\t],\n\t\"additionalProperties\": false\n}\nRecursive schemas are supported\nSample recursive schema using # to indicate root recursion.\n\n\njson\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n{\n        \"name\": \"ui\",\n        \"description\": \"Dynamically generated UI\",\n        \"strict\": true,\n        \"schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"type\": {\n                    \"type\": \"string\",\n                    \"description\": \"The type of the UI component\",\n                    \"enum\": [\"div\", \"button\", \"header\", \"section\", \"field\", \"form\"]\n                },\n                \"label\": {\n                    \"type\": \"string\",\n                    \"description\": \"The label of the UI component, used for buttons or form fields\"\n                },\n                \"children\": {\n                    \"type\": \"array\",\n                    \"description\": \"Nested UI components\",\n                    \"items\": {\n                        \"$ref\": \"#\"\n                    }\n                },\n                \"attributes\": {\n                    \"type\": \"array\",\n                    \"description\": \"Arbitrary attributes for the UI component, suitable for any element\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"name\": {\n                                \"type\": \"string\",\n                                \"description\": \"The name of the attribute, for example onClick or className\"\n                            },\n                            \"value\": {\n                                \"type\": \"string\",\n                                \"description\": \"The value of the attribute\"\n                            }\n                        },\n                      \"additionalProperties\": false,\n                      \"required\": [\"name\", \"value\"]\n                    }\n                }\n            },\n            \"required\": [\"type\", \"label\", \"children\", \"attributes\"],\n            \"additionalProperties\": false\n        }\n    }\nSample recursive schema using explicit recursion:\n\n\njson\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n{\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"linked_list\": {\n\t\t\t\"$ref\": \"#/$defs/linked_list_node\"\n\t\t}\n\t},\n\t\"$defs\": {\n\t\t\"linked_list_node\": {\n\t\t\t\"type\": \"object\",\n\t\t\t\"properties\": {\n\t\t\t\t\"value\": {\n\t\t\t\t\t\"type\": \"number\"\n\t\t\t\t},\n\t\t\t\t\"next\": {\n\t\t\t\t\t\"anyOf\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"$ref\": \"#/$defs/linked_list_node\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"type\": \"null\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"additionalProperties\": false,\n\t\t\t\"required\": [\n\t\t\t\t\"next\",\n\t\t\t\t\"value\"\n\t\t\t]\n\t\t}\n\t},\n\t\"additionalProperties\": false,\n\t\"required\": [\n\t\t\"linked_list\"\n\t]\n}\nJSON mode\nJSON mode is a more basic version of the Structured Outputs feature. While JSON mode ensures that model output is valid JSON, Structured Outputs reliably matches the model's output to the schema you specify. We recommend you use Structured Outputs if it is supported for your use case.\n\nWhen JSON mode is turned on, the model's output is ensured to be valid JSON, except for in some edge cases that you should detect and handle appropriately.\n\nTo turn on JSON mode with the Chat Completions or Assistants API you can set the response_format to { \"type\": \"json_object\" }. If you are using function calling, JSON mode is always turned on.\n\nImportant notes:\n\nWhen using JSON mode, you must always instruct the model to produce JSON via some message in the conversation, for example via your system message. If you don't include an explicit instruction to generate JSON, the model may generate an unending stream of whitespace and the request may run continually until it reaches the token limit. To help ensure you don't forget, the API will throw an error if the string \"JSON\" does not appear somewhere in the context.\nJSON mode will not guarantee the output matches any specific schema, only that it is valid and parses without errors. You should use Structured Outputs to ensure it matches your schema, or if that is not possible, you should use a validation library and potentially retries to ensure that the output matches your desired schema.\nYour application must detect and handle the edge cases that can result in the model output not being a complete JSON object (see below)\n\nEnd of selected content from https://platform.openai.com/docs/guides/structured-outputs"
    ],
    [
      "selection:1732301464467",
      "Selected content from https://script.google.com/u/0/home/projects/1sYMfeOTTFmLFASOzxtIbMuWFDnqBqppFeu94I2ZZGj9rN9_Z-l11QhqS/edit on 2024-11-22 18:51:04\n\n// Replace 'YOUR_OPENAI_API_KEY' with your actual OpenAI API key\nconst OPENAI_API_KEY = 'fake_key_replace_with_own';\n\nfunction onOpen() {\n  DocumentApp.getUi()\n    .createAddonMenu()\n    .addItem('Start AI Assistant', 'showSidebar')\n    .addToUi();\n}\n\n…    body.appendParagraph(text);\n  }\n}\n\nEnd of selected content from https://script.google.com/u/0/home/projects/1sYMfeOTTFmLFASOzxtIbMuWFDnqBqppFeu94I2ZZGj9rN9_Z-l11QhqS/edit"
    ],
    [
      "note:1732301688416",
      "----\n\nThis is the ChatGPT API with Json Response Format. With working examples. Adhere to these examples and Docs for implementing ChatGPT API clients in any form. Stick to the specification and working examples.\n\n----\n\n"
    ]
  ],
  "selectionOrder": [
    "selection:1732300532038",
    "selection:1732300549398",
    "selection:1732300603782",
    "selection:1732300675692",
    "selection:1732300748815",
    "selection:1732300786911",
    "selection:1732300799153",
    "selection:1732301022341",
    "selection:1732301037702",
    "selection:1732301063000",
    "selection:1732301176238",
    "selection:1732301464467",
    "note:1732301688416"
  ]
}